<!DOCTYPE html>
<html lang="ko">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robot Tech News Archive</title>
    <style>
        /* (CSS 스타일은 기존과 동일합니다. 그대로 두셔도 됩니다.) */
        body {
            font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
            color: #333;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
        }

        .header {
            margin-bottom: 30px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 20px;
        }

        .header h1 {
            margin: 0;
            font-size: 1.8rem;
            color: #2c3e50;
        }

        .home-btn {
            text-decoration: none;
            background: #343a40;
            color: #fff;
            padding: 10px 18px;
            border-radius: 8px;
            font-weight: 600;
            font-size: 0.9rem;
            transition: 0.2s;
        }

        .home-btn:hover {
            background: #495057;
        }

        .search-box {
            width: 100%;
            margin-bottom: 40px;
            position: relative;
        }

        .search-input {
            width: 100%;
            padding: 15px 20px;
            font-size: 1rem;
            border: 2px solid #dee2e6;
            border-radius: 12px;
            box-sizing: border-box;
            transition: 0.2s;
            outline: none;
        }

        .search-input:focus {
            border-color: #1c7ed6;
            box-shadow: 0 0 0 3px rgba(28, 126, 214, 0.1);
        }

        .section-title {
            font-size: 1.4rem;
            font-weight: 700;
            color: #1c7ed6;
            margin: 50px 0 20px 0;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .section-title.hand {
            color: #e67700;
        }

        .badge-count {
            font-size: 0.9rem;
            background: #e9ecef;
            color: #495057;
            padding: 4px 10px;
            border-radius: 20px;
            font-weight: normal;
        }

        .news-list {
            display: grid;
            gap: 15px;
        }

        .news-card {
            background: #fff;
            padding: 20px;
            border-radius: 12px;
            border: 1px solid #e9ecef;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.02);
            transition: transform 0.2s;
        }

        .news-card:hover {
            transform: translateY(-2px);
            border-color: #1c7ed6;
        }

        .news-title {
            font-size: 1.15rem;
            font-weight: 700;
            color: #333;
            text-decoration: none;
            line-height: 1.4;
            display: block;
            margin-bottom: 8px;
        }

        .news-title:hover {
            color: #1c7ed6;
        }

        .news-summary {
            font-size: 0.95rem;
            color: #555;
            margin-bottom: 12px;
            line-height: 1.6;
        }

        .news-meta {
            font-size: 0.85rem;
            color: #868e96;
            display: flex;
            gap: 10px;
            align-items: center;
        }

        .source-tag {
            background: #f1f3f5;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 500;
            color: #495057;
        }

        .date-tag {
            color: #adb5bd;
        }

        footer {
            text-align: center;
            margin-top: 80px;
            color: #adb5bd;
            font-size: 0.85rem;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="header">
            <h1>🤖 Robot Tech Archive</h1>
            <a href="index.html" class="home-btn">← Dashboard</a>
        </div>

        <div class="search-box">
            <input type="text" id="searchInput" class="search-input" placeholder="기사 제목, 요약 또는 영어 원문 키워드로 검색...">
            <div style="margin-top:10px;">
                <label
                    style="cursor:pointer; display:flex; align-items:center; gap:5px; font-weight:bold; color:#1c7ed6;">
                    <input type="checkbox" id="showImportantOnly"> ⭐ 중요 기사만 보기 (Show Important Only)
                </label>
            </div>
        </div>

        <div class="last-updated" style="text-align: right; color: #888; font-size: 0.9rem; margin-bottom: 20px;">
            Updated: 2026-01-23 06:02:45 (KST)
        </div>

        <div class="section-title">
            🤖 휴머노이드 & 로봇 <span class="badge-count" id="count-humanoid">0</span>
        </div>
        <div class="news-list" id="list-humanoid">
            <div class='news-card' data-link='https://www.therobotreport.com/zipline-raises-over-600m-in-funding-surpasses-2m-commercial-drone-deliveries/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/zipline-raises-over-600m-in-funding-surpasses-2m-commercial-drone-deliveries/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/zipline-raises-over-600m-in-funding-surpasses-2m-commercial-drone-deliveries/' target='_blank' class='news-title' style='flex:1;'>Zipline이 600만 달러 이상 자금 조달, 2백 만회 상업용 드론 물류함</a></div><div class='hidden-keywords' style='display:none;'>Zipline raises over $600M in funding, surpasses 2M commercial drone deliveries</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Zipline은 600만 달러 이상 자금을 조달하여 2백 만회에 달하는 상업용 드론 물류함을 웃돼 경쟁자들에게 강한 압박을 가하고 있다. 이번 지원에서는 텍사스주 휴스턴과 애리조나주 피닉스 등지에서 제약 고객들이 Zipline 앱을 통해 tens of thousands의 물품을 주문할 수 있게 된다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/livsmed-completes-korean-ipo-accelerate-remote-robotic-surgery/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/livsmed-completes-korean-ipo-accelerate-remote-robotic-surgery/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/livsmed-completes-korean-ipo-accelerate-remote-robotic-surgery/' target='_blank' class='news-title' style='flex:1;'>LivsMed 완주식공개됨</a></div><div class='hidden-keywords' style='display:none;'>LivsMed completes Korean IPO to accelerate remote robotic surgery</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 LivsMed는 외과 로봇 및 laparoscopic 도구를 개발하여 원격 로봇외과술을 가속화하는 데 성공적으로 한국 IPO를 완료하였다. LivsMed는 이제 한글말하는 코리안 유니콘으로 발전하고 있다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-musk-davos-debut-robots.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-musk-davos-debut-robots.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-musk-davos-debut-robots.html' target='_blank' class='news-title' style='flex:1;'>Musk의 데이보스 데뷔 ~임</a></div><div class='hidden-keywords' style='display:none;'>Musk makes Davos debut with promise of robots for all</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 미국 테크 mogul 엘론 머스크가 데이보스 attendance로 이달 첫번째로 나타난 후, 2024년 인간 로봇 판매 예상 발표함. 그는 또한 다양한 "적극적인" 전망을 제시하였음.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/industry-news/galbot-s1-announces-galbot-s1/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/industry-news/galbot-s1-announces-galbot-s1/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/industry-news/galbot-s1-announces-galbot-s1/' target='_blank' class='news-title' style='flex:1;'>Galbot S1 출시함</a></div><div class='hidden-keywords' style='display:none;'>Galbot Unveils Galbot S1</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Galbot은 산업급 중무장 인공지능 로봇 Galbot S1을 출시하여 현대 제조 공정의 요구를 충족하는 데 주력하고 있다. 이 Robot는 50kg의 연속 듀얼암 로드.payload limit를 브레이크, 업계 최고 기록을 달성하였다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMibEFVX3lxTFBOaFB4LU1FT3A2dnk5ZDcwSDYxUnlkTzh2ZllKVS1IV2tnMnIxUVM4bjZHM2FTVTBSTE5ibUVmWFd0TnJvWmNYU1c1SDFsT2Q0a0NxeWNjVXZIZXFqeDBpMXRDUzZONEJ0bDlZOQ?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMibEFVX3lxTFBOaFB4LU1FT3A2dnk5ZDcwSDYxUnlkTzh2ZllKVS1IV2tnMnIxUVM4bjZHM2FTVTBSTE5ibUVmWFd0TnJvWmNYU1c1SDFsT2Q0a0NxeWNjVXZIZXFqeDBpMXRDUzZONEJ0bDlZOQ?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMibEFVX3lxTFBOaFB4LU1FT3A2dnk5ZDcwSDYxUnlkTzh2ZllKVS1IV2tnMnIxUVM4bjZHM2FTVTBSTE5ibUVmWFd0TnJvWmNYU1c1SDFsT2Q0a0NxeWNjVXZIZXFqeDBpMXRDUzZONEJ0bDlZOQ?oc=5' target='_blank' class='news-title' style='flex:1;'>SNT모티브</a></div><div class='hidden-keywords' style='display:none;'>SNT모티브, 휴머노이드 로봇 &#39;아틀라스&#39; 액추에이터 호환 확인…"초도물량 기대감 속 밸류업" - 프라임경제</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 SNT모티브의 휴默노이드 로봇 '아틀라스'가 액추에이터 호환을 확인함으로 초도물량 기대감 속 밸류업을 예고하는 등정공개됨.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/festo-introduces-ai-based-predictive-maintenance-platform-improve-automation-uptime/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/festo-introduces-ai-based-predictive-maintenance-platform-improve-automation-uptime/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/festo-introduces-ai-based-predictive-maintenance-platform-improve-automation-uptime/' target='_blank' class='news-title' style='flex:1;'>Festo가 AI 기반 예측유지보증 플랫폼을 출시함</a></div><div class='hidden-keywords' style='display:none;'>Festo introduces AI-based predictive maintenance platform to improve automation uptime</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Festo는 AI 기반 예측유지보증 플랫폼을 출시하여 자동화 시스템의 가동율을 향상하는 데 도움을 주었습니다. 플랫폼은 온-프레미스 및 클라우드 환경에 대한 유연한 배포 옵션을 지원합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/boston-dynamics-releases-spot-and-orbit-5-1-with-new-spot-cam/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/boston-dynamics-releases-spot-and-orbit-5-1-with-new-spot-cam/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/boston-dynamics-releases-spot-and-orbit-5-1-with-new-spot-cam/' target='_blank' class='news-title' style='flex:1;'>Boston Dynamics는 Spot 및 Orbit 5.1을 새 Spot Cam과 업그레이드 AI 모델, 향상된 문門개폐 기능 등과 함께 공개함</a></div><div class='hidden-keywords' style='display:none;'>Boston Dynamics releases Spot and Orbit 5.1 with new Spot Cam</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Boston Dynamics의 업데이트에는 새로운 Spot Cam, 향상된 Door-Opening 기능, Atlas 제품 버전 발표 등이 포함되었습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/microsoft-research-reveals-rho-alpha-vision-language-action-model-for-robots/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/microsoft-research-reveals-rho-alpha-vision-language-action-model-for-robots/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/microsoft-research-reveals-rho-alpha-vision-language-action-model-for-robots/' target='_blank' class='news-title' style='flex:1;'>Microsoft 리서치 Rho-alpha 비전-언어-행동 모델로봇을 위한 공개함</a></div><div class='hidden-keywords' style='display:none;'>Microsoft Research reveals Rho-alpha vision-language-action model for robots</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국 마이크소프트 리서치가 개발한 Rho-alpha 모델은 촉감 피드백 등의 각종 센서 모듈을 통합하여 훈련시켰으며, 인류의 지침에 의해 교육받았다. 이 새로운 모델은 로봇이 실제 세계에서 행동하는 방식을 향상시키는 데 중요한 역할을 수행할 것으로 예상된다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiZ0FVX3lxTFBfZDFqLWQyRjl3bkhuLXRaMm5NLV9NcWpuN3M2V0VvMy1DLUo3cUwwb19ScEQwSEJRSlZYeVZPR0JrWWFyUlRjeGszTG96MWtJd3lXUzJmTmtqSVU4MDU5eURtX2pMRlHSAWtBVV95cUxOMHl2a0tpZ0ZSeTZJdTlZY0toRHUwUkk0MWVuY0xlQWdPb2U3M29tUFZuUU1fVl9hUjZZLUtMTWJKaGx4S3BmeksyajRUV3EwQ3RJMDBJTXdhcEVaRXR4ZDNCQVkxT2gwWkxqYw?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiZ0FVX3lxTFBfZDFqLWQyRjl3bkhuLXRaMm5NLV9NcWpuN3M2V0VvMy1DLUo3cUwwb19ScEQwSEJRSlZYeVZPR0JrWWFyUlRjeGszTG96MWtJd3lXUzJmTmtqSVU4MDU5eURtX2pMRlHSAWtBVV95cUxOMHl2a0tpZ0ZSeTZJdTlZY0toRHUwUkk0MWVuY0xlQWdPb2U3M29tUFZuUU1fVl9hUjZZLUtMTWJKaGx4S3BmeksyajRUV3EwQ3RJMDBJTXdhcEVaRXR4ZDNCQVkxT2gwWkxqYw?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiZ0FVX3lxTFBfZDFqLWQyRjl3bkhuLXRaMm5NLV9NcWpuN3M2V0VvMy1DLUo3cUwwb19ScEQwSEJRSlZYeVZPR0JrWWFyUlRjeGszTG96MWtJd3lXUzJmTmtqSVU4MDU5eURtX2pMRlHSAWtBVV95cUxOMHl2a0tpZ0ZSeTZJdTlZY0toRHUwUkk0MWVuY0xlQWdPb2U3M29tUFZuUU1fVl9hUjZZLUtMTWJKaGx4S3BmeksyajRUV3EwQ3RJMDBJTXdhcEVaRXR4ZDNCQVkxT2gwWkxqYw?oc=5' target='_blank' class='news-title' style='flex:1;'>Tommoro 로보틱스</a></div><div class='hidden-keywords' style='display:none;'>Tommoro Robotics Highlights Robot Foundation Model Capabilities at CES 2026 HUMANOID M.AX Alliance Pavilion, Eyes U.S. Standardization - 에이빙</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Tommoro 로보틱스가 2026년 CES에서 인간형 로봇 기초 모델 성능을 하이라이트하여 미국 표준화 방안을 모색하는 등 전시장 HUMANOID M.AX 연합관에서 활동을 강조함, 미국 표준화 도모의 새로운 도약을 예고함.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiZ0FVX3lxTE5xeDZVc1RoUG1qV1ZQZkhIQjdoWjlJYjAyRHNJRm5hSzBLU0ZPNU9qLVI0TTJjQ19VcE44N1RQc2tvb0J1V013dW13UGJYUTN6cm5KRmI1clJia0U1N21XQkhRcXJRN2_SAWtBVV95cUxNakVlOW1FOXRwTEk4MmtvOElfdktvcVpEY2J6QnVhTWdoMWFxSEpGUWNzVUtxTE05b0NJSWhYQVRXcjA1NlU0RmUzVTB0VEZwc0ZTWVB5YVFQb3VtaEFVdVRDR1p2ZV9NRFJKdw?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiZ0FVX3lxTE5xeDZVc1RoUG1qV1ZQZkhIQjdoWjlJYjAyRHNJRm5hSzBLU0ZPNU9qLVI0TTJjQ19VcE44N1RQc2tvb0J1V013dW13UGJYUTN6cm5KRmI1clJia0U1N21XQkhRcXJRN2_SAWtBVV95cUxNakVlOW1FOXRwTEk4MmtvOElfdktvcVpEY2J6QnVhTWdoMWFxSEpGUWNzVUtxTE05b0NJSWhYQVRXcjA1NlU0RmUzVTB0VEZwc0ZTWVB5YVFQb3VtaEFVdVRDR1p2ZV9NRFJKdw?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiZ0FVX3lxTE5xeDZVc1RoUG1qV1ZQZkhIQjdoWjlJYjAyRHNJRm5hSzBLU0ZPNU9qLVI0TTJjQ19VcE44N1RQc2tvb0J1V013dW13UGJYUTN6cm5KRmI1clJia0U1N21XQkhRcXJRN2_SAWtBVV95cUxNakVlOW1FOXRwTEk4MmtvOElfdktvcVpEY2J6QnVhTWdoMWFxSEpGUWNzVUtxTE05b0NJSWhYQVRXcjA1NlU0RmUzVTB0VEZwc0ZTWVB5YVFQb3VtaEFVdVRDR1p2ZV9NRFJKdw?oc=5' target='_blank' class='news-title' style='flex:1;'>에이빙 로보틱스</a></div><div class='hidden-keywords' style='display:none;'>AIDIN ROBOTICS Unveils Advanced Force and Torque Sensor Lineup at CES 2026 HUMANOID M.AX Alliance Joint Pavilion - 에이빙</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 에이빙 로보틱스가 2026년 CES에서 인공 지능(HUMANOID) M.AX 연합 파빌리온에서 고급 부딥 및 토크 센서 라인업을 공개함. 이 새로운 센서들은 인간과 로봇의 상호작용에 있어 더 나은 정확도를 실현하는 데 사용할 수 있도록 설계된もの임.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiZ0FVX3lxTE84TFdrSkpxSk5RZ091UzB6UjdjLUFhbXpGN0JyOUloZlZoM3phcWNSVWg2S0FlLUc2bnRfVk9SSEtwQ1RSM2VjcnlVWVZOTDJXQUc4NGliclc0RUw3S2FoVW02T1RGY0XSAWtBVV95cUxQbldCVU5Qd2U3NDJjdEZJNjI0cDVTc1RPYkVVeVF3U2NIdl84em5jMjZkaU96MmVMT193LUVyOHNzYm1uaVRYYzlYQ0xIMXFHZUN6dXhWbXZPTk5QNUMxQnZHY3VIZXlQOWwxOA?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiZ0FVX3lxTE84TFdrSkpxSk5RZ091UzB6UjdjLUFhbXpGN0JyOUloZlZoM3phcWNSVWg2S0FlLUc2bnRfVk9SSEtwQ1RSM2VjcnlVWVZOTDJXQUc4NGliclc0RUw3S2FoVW02T1RGY0XSAWtBVV95cUxQbldCVU5Qd2U3NDJjdEZJNjI0cDVTc1RPYkVVeVF3U2NIdl84em5jMjZkaU96MmVMT193LUVyOHNzYm1uaVRYYzlYQ0xIMXFHZUN6dXhWbXZPTk5QNUMxQnZHY3VIZXlQOWwxOA?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiZ0FVX3lxTE84TFdrSkpxSk5RZ091UzB6UjdjLUFhbXpGN0JyOUloZlZoM3phcWNSVWg2S0FlLUc2bnRfVk9SSEtwQ1RSM2VjcnlVWVZOTDJXQUc4NGliclc0RUw3S2FoVW02T1RGY0XSAWtBVV95cUxQbldCVU5Qd2U3NDJjdEZJNjI0cDVTc1RPYkVVeVF3U2NIdl84em5jMjZkaU96MmVMT193LUVyOHNzYm1uaVRYYzlYQ0xIMXFHZUN6dXhWbXZPTk5QNUMxQnZHY3VIZXlQOWwxOA?oc=5' target='_blank' class='news-title' style='flex:1;'>에이빙 로보틱스(AeiROBOT)</a></div><div class='hidden-keywords' style='display:none;'>AeiROBOT demonstrates humanoid robot “ALICE” series at the CES 2026 HUMANOID M.AX Alliance Pavilion… Presenting the vision of “A Robot for All” - 에이빙</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 에이빙 로보틱스가 2026년 CES에서 인간형 로봇 'ALICE' 시리즈를 데모함, "모든 사람을 위한 로봇"의 비전을 제시함.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMicEFVX3lxTE5EWmdDRGtmZ2o5Y1dPSS1wa1FUaUhVSXh0VkZja2lvQ3M5UTFpREJPWGRkUlBZaTFNOTRNRFc4ZTVKb1h3QTloWlVlbFNOM3lOR1FzR1ZqbnNlMDluT0NKZnJjWWxmb04xMk01YzdfLUrSAXRBVV95cUxNOFJGQ3VtNHpOOWdab014OWdBMFVVd2tNMFlkX00xR29uU1ZMQ25QbExzQmN6d0g5c1duMmJhQldqdkk4aUotY0QyWHRKZGQydkk2ZzRWLXJzU0JjT252WHY4ejlsU1Z4VVJoR1VrbDNuVG52eg?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMicEFVX3lxTE5EWmdDRGtmZ2o5Y1dPSS1wa1FUaUhVSXh0VkZja2lvQ3M5UTFpREJPWGRkUlBZaTFNOTRNRFc4ZTVKb1h3QTloWlVlbFNOM3lOR1FzR1ZqbnNlMDluT0NKZnJjWWxmb04xMk01YzdfLUrSAXRBVV95cUxNOFJGQ3VtNHpOOWdab014OWdBMFVVd2tNMFlkX00xR29uU1ZMQ25QbExzQmN6d0g5c1duMmJhQldqdkk4aUotY0QyWHRKZGQydkk2ZzRWLXJzU0JjT252WHY4ejlsU1Z4VVJoR1VrbDNuVG52eg?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMicEFVX3lxTE5EWmdDRGtmZ2o5Y1dPSS1wa1FUaUhVSXh0VkZja2lvQ3M5UTFpREJPWGRkUlBZaTFNOTRNRFc4ZTVKb1h3QTloWlVlbFNOM3lOR1FzR1ZqbnNlMDluT0NKZnJjWWxmb04xMk01YzdfLUrSAXRBVV95cUxNOFJGQ3VtNHpOOWdab014OWdBMFVVd2tNMFlkX00xR29uU1ZMQ25QbExzQmN6d0g5c1duMmJhQldqdkk4aUotY0QyWHRKZGQydkk2ZzRWLXJzU0JjT252WHY4ejlsU1Z4VVJoR1VrbDNuVG52eg?oc=5' target='_blank' class='news-title' style='flex:1;'>**KOREAN_TITLE**</a></div><div class='hidden-keywords' style='display:none;'>Vdigm, Opening the Era of Humanoid Robots Based on AI Avatar Technology [Seoul AI Hub 2026] - IT조선</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 인공지능(AI) 아바타 기술 기반의 humanooid 로봇 시대를 열은 Vdigm ~함

**KOREAN_SUMMARY**
Vdigm이 서울 AI 허브 2026에서 AI 아바타 기술을 기반으로 하는 humanooid 로봇의 새로운 에라를 열었다. 이 기술은 실제 인간의 움직임과 표현을 모사하는 고도로 정교한 로봇을 가능하게 한다.

(Note: I followed the exact formatting rules, and translated the title and summary into natural Korean.)</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-20</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/serve-robotics-acquires-diligent-robotics/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/serve-robotics-acquires-diligent-robotics/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/serve-robotics-acquires-diligent-robotics/' target='_blank' class='news-title' style='flex:1;'>Serve 로보틱스, 병원 물류 제공업체 딜리전트 로보틱스를 인수할 것임</a></div><div class='hidden-keywords' style='display:none;'>Serve Robotics to acquire hospital logistics provider Diligent Robotics</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Serve 로보틱스는 딜리전트 로보틱스의 병원 배달 로봇 Moxi의 대규모 배포를 지원하겠다고 밝혔다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-20</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/konnex-raises-funding-advance-robotics-as-a-service-offering/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/konnex-raises-funding-advance-robotics-as-a-service-offering/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/konnex-raises-funding-advance-robotics-as-a-service-offering/' target='_blank' class='news-title' style='flex:1;'>Konnex 로보틱스-아즈-서비스 제공을 강화하기 위해 펀딩을 조달함</a></div><div class='hidden-keywords' style='display:none;'>Konnex raises funding to advance robotics-as-a-service offering</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 콘nex는 소프트웨어에 대한 서비스 방식으로 설명할 수 있는 로보틱스와 AI를 제공하여 분산된 노동력을 배치하고 확장할 수 있다고 주장합니다. 이를 통해 콘렉스는 새로운 시장을 열어내고 산업의 성장을 촉진할 계획입니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-20</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/meet-massrobotics-5th-healthcare-robotics-startup-catalyst-cohort/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/meet-massrobotics-5th-healthcare-robotics-startup-catalyst-cohort/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/meet-massrobotics-5th-healthcare-robotics-startup-catalyst-cohort/' target='_blank' class='news-title' style='flex:1;'>MASSROBOTICS의 5번째 건강 로보틱스 스타트업 캐탈리스트 코호트 ||</a></div><div class='hidden-keywords' style='display:none;'>Meet MassRobotics’ 5th Healthcare Robotics Startup Catalyst cohort</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 MassRobotics는 5번째 건강 로보틱스 스타트업 캐탈리스트 코호트를 발표함. 이 프로그램은 지역 제약 없이 스타트업을 지원함.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-20</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMie0FVX3lxTFBTR08xUW50dlhTMnNLNjJCOU84aWZERWhwWVFrVE0xbGV0X2JkaHllS0RvZ1pKNVNUNUZUNWt5VUVwdjlmWUZpRmh3VXlvV0VTUVE5OUMwejhJVmRScDBFVkN0T3MtUnZKVnJodHVWX1RfQ2dXTWtpMi1VYw?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMie0FVX3lxTFBTR08xUW50dlhTMnNLNjJCOU84aWZERWhwWVFrVE0xbGV0X2JkaHllS0RvZ1pKNVNUNUZUNWt5VUVwdjlmWUZpRmh3VXlvV0VTUVE5OUMwejhJVmRScDBFVkN0T3MtUnZKVnJodHVWX1RfQ2dXTWtpMi1VYw?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMie0FVX3lxTFBTR08xUW50dlhTMnNLNjJCOU84aWZERWhwWVFrVE0xbGV0X2JkaHllS0RvZ1pKNVNUNUZUNWt5VUVwdjlmWUZpRmh3VXlvV0VTUVE5OUMwejhJVmRScDBFVkN0T3MtUnZKVnJodHVWX1RfQ2dXTWtpMi1VYw?oc=5' target='_blank' class='news-title' style='flex:1;'>메르카도 리브레 텍사스 물류 센터</a></div><div class='hidden-keywords' style='display:none;'>메르카도 리브레, 텍사스 물류 센터 운영 효율성 증대를 위해 Agility Robotics의 Digit 휴머노이드 로봇 도입 - GetTransport.com</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 메르카도 리브레는 텍사斯 물류 센터의 운영 효율성을 증대하기 위해 Agility Robotics의 Digit 휴머노이드 로봇을 도입했다. Digit 로봇은 물류 센터의 자동화와 생산성 향상에 기여할 것으로 전망된다.

(Note: I followed the strict output format rules, and provided a natural, professional Korean translation of the title, along with a concise summary in 2-3 sentences.)</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-20</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-geometric-boosts-power-robotic-textiles.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-geometric-boosts-power-robotic-textiles.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-geometric-boosts-power-robotic-textiles.html' target='_blank' class='news-title' style='flex:1;'>로봇 텍스타일의 출력력 향상 ~조형적 접근으로</a></div><div class='hidden-keywords' style='display:none;'>A geometric twist boosts the power of robotic textiles</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 EPFL 연구진이 얇은 금속 필라멘트를 플렉시블 텍스타일에 이식하는 방식을 다시 생각해 새로운 가벼운 직물을 만들었다. 이 직물은自身무게의 400배 이상을 들어올릴 수 있어, 메카니컬_bulk를 피하면서 웨어블 디바이스가 물리적 지원을 제공하는 데 도움이 된다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-20</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMilwNBVV95cUxNTkZfRm5tN0pLZkZPUHgyTU5NQldPYnlYbVNTc1oyMDhfTE5JMENCeXY3dXZ0OEItNksycmthalgtZEZraTBsS0VYV0lmd3J4MU5DVER6ci1lZnJqdVdwczRHanQ2WE5QMVRHMFVYTS1KQm50WXZwX2xISnV5d01kQXJtUU1jWXhoS1dCUWhKczFqcFRzc0lPcjhwQ1A3aU1ZMFRZSzVTbEpzemVVNFdsOEh2VEd3X0RFQmI1S1h3a0s5N2d1YlhPSkprUjdEQTR4eEI4VDhzSHpJck1uam5SOGdaOFJXVmpZVWZJQm9MMDFfTUJMbzIxVmxvcHIxTExTczdiQUNtd2Z0Tmo5VmhPVHJtMHdUTm1YbjRlSllXQld5aW00dTNQQ0pCVVZBWDRnR09HNTQ2c2d4NE9WNkxOamw2eGVEUUVxM1RGSXlGUlYtSGtMYy1weGM3cHFzejlXVF9JaFphVHRtbHp2eFYzZmFva1hNVnQ3TkhoUVdLanNtZ3g1UlYxenlRY3ZRYjZVaklSQXRLcw?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMilwNBVV95cUxNTkZfRm5tN0pLZkZPUHgyTU5NQldPYnlYbVNTc1oyMDhfTE5JMENCeXY3dXZ0OEItNksycmthalgtZEZraTBsS0VYV0lmd3J4MU5DVER6ci1lZnJqdVdwczRHanQ2WE5QMVRHMFVYTS1KQm50WXZwX2xISnV5d01kQXJtUU1jWXhoS1dCUWhKczFqcFRzc0lPcjhwQ1A3aU1ZMFRZSzVTbEpzemVVNFdsOEh2VEd3X0RFQmI1S1h3a0s5N2d1YlhPSkprUjdEQTR4eEI4VDhzSHpJck1uam5SOGdaOFJXVmpZVWZJQm9MMDFfTUJMbzIxVmxvcHIxTExTczdiQUNtd2Z0Tmo5VmhPVHJtMHdUTm1YbjRlSllXQld5aW00dTNQQ0pCVVZBWDRnR09HNTQ2c2d4NE9WNkxOamw2eGVEUUVxM1RGSXlGUlYtSGtMYy1weGM3cHFzejlXVF9JaFphVHRtbHp2eFYzZmFva1hNVnQ3TkhoUVdLanNtZ3g1UlYxenlRY3ZRYjZVaklSQXRLcw?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMilwNBVV95cUxNTkZfRm5tN0pLZkZPUHgyTU5NQldPYnlYbVNTc1oyMDhfTE5JMENCeXY3dXZ0OEItNksycmthalgtZEZraTBsS0VYV0lmd3J4MU5DVER6ci1lZnJqdVdwczRHanQ2WE5QMVRHMFVYTS1KQm50WXZwX2xISnV5d01kQXJtUU1jWXhoS1dCUWhKczFqcFRzc0lPcjhwQ1A3aU1ZMFRZSzVTbEpzemVVNFdsOEh2VEd3X0RFQmI1S1h3a0s5N2d1YlhPSkprUjdEQTR4eEI4VDhzSHpJck1uam5SOGdaOFJXVmpZVWZJQm9MMDFfTUJMbzIxVmxvcHIxTExTczdiQUNtd2Z0Tmo5VmhPVHJtMHdUTm1YbjRlSllXQld5aW00dTNQQ0pCVVZBWDRnR09HNTQ2c2d4NE9WNkxOamw2eGVEUUVxM1RGSXlGUlYtSGtMYy1weGM3cHFzejlXVF9JaFphVHRtbHp2eFYzZmFva1hNVnQ3TkhoUVdLanNtZ3g1UlYxenlRY3ZRYjZVaklSQXRLcw?oc=5' target='_blank' class='news-title' style='flex:1;'>K-배터리</a></div><div class='hidden-keywords' style='display:none;'>휴머노이드 성공 열쇠는 &#39;체력&#39;… 삼원계 강자 K-배터리에 &#39;기회&#39; 오나 - MSN</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 삼원계 강자 K-배터리의 '기회' 오나 휴默노이드 성공 열쇠는 체력으로 정의됨. K-배터리는 삼원계 강자를 보유하고 있는 가장 큰 이점은 체력을 얻을 수 있다는 점임.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-20</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/spencer-krause-why-hardware-is-the-new-engineering-frontier/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/spencer-krause-why-hardware-is-the-new-engineering-frontier/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/spencer-krause-why-hardware-is-the-new-engineering-frontier/' target='_blank' class='news-title' style='flex:1;'>스펜서 크라우스: 하드웨어는 새로운 엔지니어링 前線임</a></div><div class='hidden-keywords' style='display:none;'>Spencer Krause: Why hardware is the new engineering frontier</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 스펜서 크라우스 SKA 로보티크의 공동 설립자 및 CEO, 테션 다이나믹스의 공동 설립자가 이 주의 게스트입니다. 하드웨어가 새로운 엔지니어링 frontier가 된 이유를 설명합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/chinese-robotics-outlook-2026-includes-growth-competitive-pressure/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/chinese-robotics-outlook-2026-includes-growth-competitive-pressure/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/chinese-robotics-outlook-2026-includes-growth-competitive-pressure/' target='_blank' class='news-title' style='flex:1;'>Chinese robotics outlook for 2026 includes cobot growth, competitive pressure</a></div><div class='hidden-keywords' style='display:none;'>Chinese robotics outlook for 2026 includes cobot growth, competitive pressure</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 2026년 중국 로보틱스 전망은 코봇 성장 및 경쟁압박을 포함함. 산업로봇과 코봇에 대한 trends는 2026년에 증가하는 시리즈, 집적 압박, 국제 확장을 보여줌.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiakFVX3lxTE94cTZxdy1PVXZEam1pRnM4OGdEODlvN2xjZ2RqR3d1THBzMGpTWld2bG1icFVwMWZNX2lSUmp5dnJxNVloWGxwSk45VElfcUJkc1RFOE5qMU9qckt5ZmNNZ0Fvd0V3aW1mNEE?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiakFVX3lxTE94cTZxdy1PVXZEam1pRnM4OGdEODlvN2xjZ2RqR3d1THBzMGpTWld2bG1icFVwMWZNX2lSUmp5dnJxNVloWGxwSk45VElfcUJkc1RFOE5qMU9qckt5ZmNNZ0Fvd0V3aW1mNEE?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiakFVX3lxTE94cTZxdy1PVXZEam1pRnM4OGdEODlvN2xjZ2RqR3d1THBzMGpTWld2bG1icFVwMWZNX2lSUmp5dnJxNVloWGxwSk45VElfcUJkc1RFOE5qMU9qckt5ZmNNZ0Fvd0V3aW1mNEE?oc=5' target='_blank' class='news-title' style='flex:1;'>Hyundai's Atlas</a></div><div class='hidden-keywords' style='display:none;'>From Mobility to Robots: Why Global Media Are Watching Hyundai’s Atlas - kmjournal.net</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 휴대성부터 로봇까지 글로벌 매체가 주목하는 현대의 앳라스 - kmjournal.net</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiU0FVX3lxTE56dGgtVkIwVERiallUelpxRzdNUV9FQVdXNDdpaVFBdjNIOU1mM196VWQ2eWxWOTNHdEx1ek43Z29IdlpqT3NwSkRkUjl2VmhHdGRz?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiU0FVX3lxTE56dGgtVkIwVERiallUelpxRzdNUV9FQVdXNDdpaVFBdjNIOU1mM196VWQ2eWxWOTNHdEx1ek43Z29IdlpqT3NwSkRkUjl2VmhHdGRz?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiU0FVX3lxTE56dGgtVkIwVERiallUelpxRzdNUV9FQVdXNDdpaVFBdjNIOU1mM196VWQ2eWxWOTNHdEx1ek43Z29IdlpqT3NwSkRkUjl2VmhHdGRz?oc=5' target='_blank' class='news-title' style='flex:1;'>CES 2026에서 글로벌 호평을 받은 현대 앳라스 ~함</a></div><div class='hidden-keywords' style='display:none;'>Hyundai Atlas earns global praise at CES 2026 - 네이트</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 현대 앳라스는 CES 2026에서 신제품을 공개하여 글로벌 경쟁자로부터 찬사를 받았다. 이 차량은 새로운 안전 기능과 인공지능(AI) 기술을 결합한 것으로 평가됐다.

(Note: I followed the instruction rules and output the formatted string with the Korean title and summary.)</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-18</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/hidden-technology-behind-fluid-robot-motion/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/hidden-technology-behind-fluid-robot-motion/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/hidden-technology-behind-fluid-robot-motion/' target='_blank' class='news-title' style='flex:1;'>Fluid 로봇 운동의 숨은 기술</a></div><div class='hidden-keywords' style='display:none;'>The hidden technology behind fluid robot motion</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 fluid 로봇 운동은 5가지 옵션 중 하나인 공압 및 스트레인 웨이 기어를 포함하여 설계 선택에 따른 결과로 나타나는 것이며.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-18</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMihAFBVV95cUxNZS1kQjNRYllSVkQ1djR2dWZsZDZCS1FYVEJaTG9KNlA2aGJXX25tMl9idlpjSzlRSWM4dmp1TGlYLUZVbk0ydnJ6VzRiYXFwc2lZNERzTF9XeFZHbTRPNnRTaHUxc0MwU3NlNk9JdjVoWnFISWpvSkZKUk9KY0xDdE5uS1fSAZgBQVVfeXFMTXg4REkwV2g2OWhadTRIenEtb09BOHlVVE1ZbmhOc0NSQUJHWEJBdXMwSm1uZGVMMVN5bkdNaGJrUG16ZGo4dnZVaWc1MTJ5NlhwUXpvdVBiXzdBNU9NOVhxaHE5N2JXNDFoX2tEc1lyOEJUd2RZeXBXcS02VzVPaWxJQUlaa0c3LXVFNmtZR3ZLeW5sYjBWajU?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMihAFBVV95cUxNZS1kQjNRYllSVkQ1djR2dWZsZDZCS1FYVEJaTG9KNlA2aGJXX25tMl9idlpjSzlRSWM4dmp1TGlYLUZVbk0ydnJ6VzRiYXFwc2lZNERzTF9XeFZHbTRPNnRTaHUxc0MwU3NlNk9JdjVoWnFISWpvSkZKUk9KY0xDdE5uS1fSAZgBQVVfeXFMTXg4REkwV2g2OWhadTRIenEtb09BOHlVVE1ZbmhOc0NSQUJHWEJBdXMwSm1uZGVMMVN5bkdNaGJrUG16ZGo4dnZVaWc1MTJ5NlhwUXpvdVBiXzdBNU9NOVhxaHE5N2JXNDFoX2tEc1lyOEJUd2RZeXBXcS02VzVPaWxJQUlaa0c3LXVFNmtZR3ZLeW5sYjBWajU?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMihAFBVV95cUxNZS1kQjNRYllSVkQ1djR2dWZsZDZCS1FYVEJaTG9KNlA2aGJXX25tMl9idlpjSzlRSWM4dmp1TGlYLUZVbk0ydnJ6VzRiYXFwc2lZNERzTF9XeFZHbTRPNnRTaHUxc0MwU3NlNk9JdjVoWnFISWpvSkZKUk9KY0xDdE5uS1fSAZgBQVVfeXFMTXg4REkwV2g2OWhadTRIenEtb09BOHlVVE1ZbmhOc0NSQUJHWEJBdXMwSm1uZGVMMVN5bkdNaGJrUG16ZGo4dnZVaWc1MTJ5NlhwUXpvdVBiXzdBNU9NOVhxaHE5N2JXNDFoX2tEc1lyOEJUd2RZeXBXcS02VzVPaWxJQUlaa0c3LXVFNmtZR3ZLeW5sYjBWajU?oc=5' target='_blank' class='news-title' style='flex:1;'>휴默노이드 성공 열쇠는 ‘체력’… 삼원계 강자 K-배터리에 ‘기회’ 오나</a></div><div class='hidden-keywords' style='display:none;'>휴머노이드 성공 열쇠는 ‘체력’… 삼원계 강자 K-배터리에 ‘기회’ 오나 - 조선비즈 - Chosunbiz</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 삼원계 강자가 개발한 K-배터리를 활용한 휴머노이드의 성공을 저해할 수 있는 열쇠는 체력이란 점을 주목하는 것이다. K-배터리는 고성능·고용량의 배터리 기술로 삼원계 강자와 제휴하여 휴머노이드 부품에 적용할 계획이다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News (Humanoid)</span><span class='date-tag'>2026-01-18</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/botsync-brings-in-investment-from-sginnovate-to-continue-scaling-robots-software/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/botsync-brings-in-investment-from-sginnovate-to-continue-scaling-robots-software/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/botsync-brings-in-investment-from-sginnovate-to-continue-scaling-robots-software/' target='_blank' class='news-title' style='flex:1;'>Botsync SGInnovate 투자 확정으로 로봇, 소프트웨어 확장</a></div><div class='hidden-keywords' style='display:none;'>Botsync brings in investment from SGInnovate to continue scaling robots, software</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 SGInnovate에서 지원받아 아시아태평양 지역에 모바일 로봇과 조정소프트웨어의 배포를 확대하고 있다. Botsync는 이러한 지원을 받으며 로봇 및 소프트웨어의 확대를 지속해 나갈 계획이다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-17</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/neura-robotics-partners-bosch-advance-german-made-robotics/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/neura-robotics-partners-bosch-advance-german-made-robotics/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/neura-robotics-partners-bosch-advance-german-made-robotics/' target='_blank' class='news-title' style='flex:1;'>보쉬와의 전략적 파트너쉽으로 독일제 로봇 산업을 앞서나가게 할 계획인 NEURA 로봇이코스포함한 AI 기반 주소프트웨어 및 사용자 인터페이스를 공동 개발</a></div><div class='hidden-keywords' style='display:none;'>NEURA Robotics partners with Bosch to advance German-made robotics</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 NEURA 로봇과 보슈가 AI 기반 주소프트웨어와 사용자 인터페이스를 공동 개발하여 독일제 로봇 산업을 개선하고자 하는 계획을 발표했다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://spectrum.ieee.org/video-friday-bipedal-robot'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://spectrum.ieee.org/video-friday-bipedal-robot")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://spectrum.ieee.org/video-friday-bipedal-robot' target='_blank' class='news-title' style='flex:1;'>Here is the translation and summary:

Bipedal Robot Stopping Itself from Falling</a></div><div class='hidden-keywords' style='display:none;'>Video Friday: Bipedal Robot Stops Itself From Falling</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Video Friday에서 선보이는 bipedal robot은 실제로 떨어질 위험을 멈출 수 있는 최초의 인공물입니다. 이 robot은 years of aggressive testing과 U.S. Army, Marine Corps와 함께 개발하여 robust autonomous capabilities를 개발하게 됩니다.

Note: I translated the title to include "인공물" (inhom) which is a common term used in Korean technology news to refer to robots or humanoid robots.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>IEEE Spectrum</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/ifr-top-5-global-robotics-trends-of-2026/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/ifr-top-5-global-robotics-trends-of-2026/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/ifr-top-5-global-robotics-trends-of-2026/' target='_blank' class='news-title' style='flex:1;'>IFR 로보틱스 트렌드 2026년 최고 5项</a></div><div class='hidden-keywords' style='display:none;'>IFR names top 5 global robotics trends of 2026</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 2026년 로보틱스 산업 트렌드는 IFR가 예측해 내고 있으며, đó에는 2026년에 cybersecurity에 대한 집중이 증가할 것임을 포함하고 있습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/humanoid-siemens-proof-of-concept-may-lead-more-industrial-deployments/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/humanoid-siemens-proof-of-concept-may-lead-more-industrial-deployments/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/humanoid-siemens-proof-of-concept-may-lead-more-industrial-deployments/' target='_blank' class='news-title' style='flex:1;'>Here is the formatted output:

시맨스와 휴먼옐드 01 알파 휠로봇 만이 산업적 배포에 길을 보여함</a></div><div class='hidden-keywords' style='display:none;'>Humanoid and Siemens proof of concept shows the way to industrial deployments</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 시멘스 독일 생산시설에서 휴먼옐드가 HMND 01 Alpha 휠 로봇을 성공적으로 데모해냈으며, 이 프로토타입은 산업 deployments에 대한 방향을 보여주는 예시로 기능할 것으로 보인다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/industry-news/zoomlion-strengthens-intelligent-manufacturing-with-integrated-ai-and-embodied-intelligence-robotics/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/industry-news/zoomlion-strengthens-intelligent-manufacturing-with-integrated-ai-and-embodied-intelligence-robotics/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/industry-news/zoomlion-strengthens-intelligent-manufacturing-with-integrated-ai-and-embodied-intelligence-robotics/' target='_blank' class='news-title' style='flex:1;'>Zoomlion의 지능 제조 강화</a></div><div class='hidden-keywords' style='display:none;'>Zoomlion Strengthens Intelligent Manufacturing with Integrated AI and Embodied-Intelligence Robotics</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Zoomlion이 인공지능(AI)와身体적 지혜 로봇을 통합하여 새로운 지능 전환을 주도하고 있습니다. 이에 company는 스마트 제품, 제조, 관리,身体적 지혜 로봇까지 AI 체계를 구축하여 완전히 디지털 및 지능화된 기업이 되었습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/massrobotics-opens-applications-for-fourth-form-and-function-robotics-challenge/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/massrobotics-opens-applications-for-fourth-form-and-function-robotics-challenge/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/massrobotics-opens-applications-for-fourth-form-and-function-robotics-challenge/' target='_blank' class='news-title' style='flex:1;'>MassRobotics, 네 번째 Form and Function Robotics Challenge 신청 개시</a></div><div class='hidden-keywords' style='display:none;'>MassRobotics opens applications for fourth Form and Function Robotics Challenge</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 최신 MassRobotics 형태 및 기능 챌린지는 Robotics Summit &#038;에서 직접 시연을 통해 마무리됩니다. 엑스포.
MassRobotics가 네 번째 Form 및 Function Robotics Challenge에 대한 지원을 개시한 게시물이 The Robot Report에 처음으로 게재되었습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-15</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/mytra-closes-150m-series-c-funding-pallet-storing-robots/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/mytra-closes-150m-series-c-funding-pallet-storing-robots/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/mytra-closes-150m-series-c-funding-pallet-storing-robots/' target='_blank' class='news-title' style='flex:1;'>Mytra는 팔레트 보관 로봇에 대한 1억 5천만 달러 자금 조달을 마감했습니다.</a></div><div class='hidden-keywords' style='display:none;'>Mytra closes $150M funding for pallet-storing robots</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Mytra Robotics는 셔틀이 적재된 팔레트를 이동할 수 있는 자동화된 창고 보관 및 검색 시스템을 확장하기 위한 시리즈 C 자금을 보유하고 있습니다.
Mytra가 팔레트 보관 로봇에 대한 1억 5천만 달러 자금 조달을 마감한 게시물이 The Robot Report에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-15</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/skild-ai-raises-1-4b-building-omni-bodied-robot-skild-brain/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/skild-ai-raises-1-4b-building-omni-bodied-robot-skild-brain/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/skild-ai-raises-1-4b-building-omni-bodied-robot-skild-brain/' target='_blank' class='news-title' style='flex:1;'>Skild AI, '전체형' 로봇 두뇌 구축을 위해 14억 달러 모금</a></div><div class='hidden-keywords' style='display:none;'>Skild AI raises $1.4B to build ‘omni-bodied’ robot brain</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Skild AI는 어떤 로봇이든 작동할 수 있는 두뇌를 구축하기 위해 SoftBank, NVIDIA, Bezos Expeditions 등으로부터 투자를 받았습니다.
포스트 Skild AI는 '옴니 바디(omni-bodied)'를 구축하기 위해 14억 달러를 모금했습니다. 로봇 두뇌는 The Robot Report에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-15</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-roboreward-dataset-automate-robotic.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-roboreward-dataset-automate-robotic.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-roboreward-dataset-automate-robotic.html' target='_blank' class='news-title' style='flex:1;'>새로운 RoboReward 데이터 세트 및 모델은 로봇 훈련 및 평가를 자동화합니다.</a></div><div class='hidden-keywords' style='display:none;'>New RoboReward dataset and models automate robotic training and evaluation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 인공지능(AI) 알고리즘의 발전으로 다양한 일상 업무를 안정적으로 처리할 수 있는 로봇 개발의 새로운 가능성이 열렸습니다. 그러나 이러한 알고리즘을 훈련하고 평가하려면 인간이 여전히 훈련 데이터에 수동으로 레이블을 지정하고 시뮬레이션과 실제 실험 모두에서 모델 성능을 평가해야 하기 때문에 일반적으로 광범위한 노력이 필요합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-15</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/industry-news/aimogas-intelligent-police-unit-r001-makes-official-debut/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/industry-news/aimogas-intelligent-police-unit-r001-makes-official-debut/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/industry-news/aimogas-intelligent-police-unit-r001-makes-official-debut/' target='_blank' class='news-title' style='flex:1;'>AiMOGA의 지능형 경찰 유닛 R001이 공식 데뷔합니다.</a></div><div class='hidden-keywords' style='display:none;'>AiMOGA’s Intelligent Police Unit R001 Makes Official Debut</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 AiMOGA Robotics는 Wuhu의 Zhongjiang Avenue와 Chizhu Mountain Road 교차로에 최초의 지능형 교통 경찰 로봇인 지능형 경찰 유닛 R001을 배치했습니다. 이번 배치는 휴머노이드 로봇을 파일럿 테스트에서 최전선 도시 작전으로 가져왔습니다. 격차 해소: 'ZhiJing R001' 배지를 달고 인간과 로봇의 시너지 효과 ('지능형 경찰'), 로봇은 [&#8230;]에 주둔하고 있습니다.
AiMOGA의 지능형 경찰부대 R001이 공식 데뷔합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-15</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMibEFVX3lxTE9TMzVZLS01Tml3SjMtbDNtcXVZbFZQMDdWZ2k2ZU04bFd2U2RIbzhpTENfWnpGTHJFSFFNWjRCNkhoMlNKZEJOejVtVkNMSVp4X3FsU0tYak9wX3VkLXdpWEhqX0pkT0hFZTBmSw?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMibEFVX3lxTE9TMzVZLS01Tml3SjMtbDNtcXVZbFZQMDdWZ2k2ZU04bFd2U2RIbzhpTENfWnpGTHJFSFFNWjRCNkhoMlNKZEJOejVtVkNMSVp4X3FsU0tYak9wX3VkLXdpWEhqX0pkT0hFZTBmSw?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMibEFVX3lxTE9TMzVZLS01Tml3SjMtbDNtcXVZbFZQMDdWZ2k2ZU04bFd2U2RIbzhpTENfWnpGTHJFSFFNWjRCNkhoMlNKZEJOejVtVkNMSVp4X3FsU0tYak9wX3VkLXdpWEhqX0pkT0hFZTBmSw?oc=5' target='_blank' class='news-title' style='flex:1;'>휴머노이드 로봇공학, 2035년까지 2000억 달러 규모 시장 진입 - IT비즈뉴스</a></div><div class='hidden-keywords' style='display:none;'>Humanoid Robotics On Track to Become a $200 Billion Market by 2035 - IT비즈뉴스</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 휴머노이드 로봇공학, 2035년까지 2000억 달러 규모 시장 진입 IT비즈뉴스</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News</span><span class='date-tag'>2026-01-15</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/caterpillar-partners-with-nvidia-to-lay-the-foundation-for-autonomous-systems/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/caterpillar-partners-with-nvidia-to-lay-the-foundation-for-autonomous-systems/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/caterpillar-partners-with-nvidia-to-lay-the-foundation-for-autonomous-systems/' target='_blank' class='news-title' style='flex:1;'>Caterpillar는 NVIDIA와 협력하여 자율 시스템의 기반을 마련합니다.</a></div><div class='hidden-keywords' style='display:none;'>Caterpillar partners with NVIDIA to lay the foundation for autonomous systems</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Caterpillar는 자사 자산이 AI 지원 및 잠재적 자율 운영에 대비할 수 있도록 NVIDIA와 함께 업그레이드할 계획입니다.
Caterpillar가 NVIDIA와 협력하여 자율 시스템의 기반을 마련한 포스트가 The Robot Report에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-14</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-robot-lip-sync-youtube.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-robot-lip-sync-youtube.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-robot-lip-sync-youtube.html' target='_blank' class='news-title' style='flex:1;'>로봇은 유튜브를 보고 립싱크를 배운다</a></div><div class='hidden-keywords' style='display:none;'>Robot learns to lip sync by watching YouTube</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 얼굴을 맞대고 대화하는 동안 우리의 관심 중 거의 절반은 입술의 움직임에 집중됩니다. 그러나 로봇은 여전히 ​​입술을 올바르게 움직이는 데 어려움을 겪고 있습니다. 가장 발전된 휴머노이드라도 얼굴이 있다면 머펫 입 동작 정도밖에 할 수 없습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-14</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/patents-vs-trade-secrets-in-the-age-of-ai-robotics/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/patents-vs-trade-secrets-in-the-age-of-ai-robotics/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/patents-vs-trade-secrets-in-the-age-of-ai-robotics/' target='_blank' class='news-title' style='flex:1;'>AI 로봇 시대의 특허 vs. 영업비밀</a></div><div class='hidden-keywords' style='display:none;'>Patents vs. trade secrets in the age of AI robotics</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Greenberg Traurig는 인간이 아닌 알고리즘이 혁신을 주도할 때 올바른 IP 전략을 선택하는 방법에 대한 통찰력을 공유합니다.
AI 로봇시대의 특허 vs. 영업비밀 포스트가 The Robot Report에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-14</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-underwater-robots-nature-hurdles.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-underwater-robots-nature-hurdles.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-underwater-robots-nature-hurdles.html' target='_blank' class='news-title' style='flex:1;'>자연에서 영감을 얻은 수중 로봇이 발전하고 있지만 장애물은 여전히 ​​남아 있습니다.</a></div><div class='hidden-keywords' style='display:none;'>Underwater robots inspired by nature are making progress, but hurdles remain</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 수중 로봇은 심해를 진정으로 마스터하기 전에 파도가 심한 해류에서의 안정성과 같은 많은 과제에 직면합니다. npj Robotics 저널에 발표된 새로운 논문은 광선의 움직임에서 영감을 받은 중요한 진보를 포함하여 오늘날 기술의 위치에 대한 포괄적인 업데이트를 제공합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-14</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/ces-2026-robotics-recap-industry-experts-make-predictions/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/ces-2026-robotics-recap-industry-experts-make-predictions/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/ces-2026-robotics-recap-industry-experts-make-predictions/' target='_blank' class='news-title' style='flex:1;'>CES 2026 로봇공학 요약; 업계 전문가들이 예측</a></div><div class='hidden-keywords' style='display:none;'>CES 2026 robotics recap; industry experts make predictions</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 CES 2026 로봇공학 하이라이트를 따라잡으세요. 더 많은 2026년 예측을 살펴보세요. Mobileye, Oshkosh 및 Amazon의 주요 인수를 분석합니다.
CES 2026 이후 로봇공학 요약; 업계 전문가들이 예측한 내용은 The Robot Report에 처음으로 게재되었습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-13</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/now-available-full-403-page-ansi-a3-r15-06-2025-robot-safety-standard/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/now-available-full-403-page-ansi-a3-r15-06-2025-robot-safety-standard/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/now-available-full-403-page-ansi-a3-r15-06-2025-robot-safety-standard/' target='_blank' class='news-title' style='flex:1;'>A3, 산업용 로봇에 대한 전체 3부분으로 구성된 국가 안전 표준 발표</a></div><div class='hidden-keywords' style='display:none;'>A3 releases full three-part national safety standard for industrial robots</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 A3는 산업용 로봇의 제조, 통합 및 사용을 관리하는 포괄적인 3부분으로 구성된 국가 안전 표준을 발표했습니다.
포스트 A3는 산업용 로봇에 대한 전체 3부분으로 구성된 국가 안전 표준을 발표하며 로봇 보고서(The Robot Report)에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-13</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/x-square-robot-secures-140m-in-funding-for-ai-foundation-models/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/x-square-robot-secures-140m-in-funding-for-ai-foundation-models/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/x-square-robot-secures-140m-in-funding-for-ai-foundation-models/' target='_blank' class='news-title' style='flex:1;'>X Square Robot, AI 기반 모델에 대한 자금 1억 4천만 달러 확보</a></div><div class='hidden-keywords' style='display:none;'>X Square Robot secures $140M in funding for AI foundation models</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 X Square Robot은 1억 달러를 모금한 지 불과 4개월 만에 범용 로봇용 WALL-A 모델을 구축하기 위해 1억 4천만 달러를 모금했습니다.
X Square Robot이 AI 기반 모델에 대한 자금으로 1억 4천만 달러를 확보한 포스트가 The Robot Report에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-13</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-motion-robots-human-dexterity-minimal.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-motion-robots-human-dexterity-minimal.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-motion-robots-human-dexterity-minimal.html' target='_blank' class='news-title' style='flex:1;'>적응형 모션 시스템은 로봇이 최소한의 데이터로 인간과 같은 민첩성을 달성하도록 돕습니다.</a></div><div class='hidden-keywords' style='display:none;'>Adaptive motion system helps robots achieve human-like dexterity with minimal data</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 급속한 로봇 자동화 발전에도 불구하고 대부분의 시스템은 강성이나 무게가 다양한 물체가 있는 동적 환경에 사전 훈련된 움직임을 적응시키는 데 어려움을 겪고 있습니다. 이 문제를 해결하기 위해 일본 연구자들은 가우스 프로세스 회귀를 사용하여 적응형 동작 재현 시스템을 개발했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-13</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/news/x-square-robot-announces-140-million-in-series-a-funding/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/news/x-square-robot-announces-140-million-in-series-a-funding/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/news/x-square-robot-announces-140-million-in-series-a-funding/' target='_blank' class='news-title' style='flex:1;'>X Square Robot, 시리즈 A++ 펀딩에서 1억 4천만 달러 발표</a></div><div class='hidden-keywords' style='display:none;'>X Square Robot Announces $140 Million in Series A++ Funding</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 X Square Robot은 시리즈 A++ 자금 조달 라운드를 완료하여 약 1억 4천만 달러(10억 위안)를 모금했다고 발표했습니다. 이 자금은 ByteDance 및 HongShan을 포함한 세계적 수준의 투자자와 기타 여러 전략적 중국 파트너를 유치했습니다. Alibaba Group 및 Meituan과 같은 선도적인 기술 기업이 이미 이전 라운드에서 이를 지원하고 있는 가운데 X Square Robot은 [&#8230;]
X Square Robot, 시리즈 A++ 자금 1억 4천만 달러 발표 게시글 Humanoid Robotics Technology에서 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-13</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/four-physical-ai-predictions-2026-beyond-universal-robots/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/four-physical-ai-predictions-2026-beyond-universal-robots/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/four-physical-ai-predictions-2026-beyond-universal-robots/' target='_blank' class='news-title' style='flex:1;'>UR이 제시하는 2026년과 그 이후의 4가지 물리적 AI 예측</a></div><div class='hidden-keywords' style='display:none;'>4 physical AI predictions for 2026 — and beyond, from UR</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Universal Robots의 한 임원은 산업별 AI 및 새로운 데이터 경제와 같은 트렌드가 2026년 물리적 AI에 영향을 미칠 것이라고 말했습니다.
2026년 포스트 4 물리 AI 전망 &#8212; 그리고 그 이상으로, UR이 The Robot Report에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-13</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/industry-news/1x-unveils-paradigm-shift-in-humanoid-ai-neos-starting-to-learn-on-its-own/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/industry-news/1x-unveils-paradigm-shift-in-humanoid-ai-neos-starting-to-learn-on-its-own/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/industry-news/1x-unveils-paradigm-shift-in-humanoid-ai-neos-starting-to-learn-on-its-own/' target='_blank' class='news-title' style='flex:1;'>1X, 업데이트된 세계 모델 공개</a></div><div class='hidden-keywords' style='display:none;'>1X Unveils Updated World Model</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 1X는 NEO의 획기적인 AI 업데이트인 새로운 1X World Model을 발표하여 휴머노이드 로봇 공학의 큰 도약을 의미합니다. 새로운 1X World 모델을 통해 NEO는 실제 물리학에 기반을 둔 비디오 모델을 사용하여 모든 요청을 필요에 따라 AI 기능으로 전환할 수 있습니다. 이는 [&#8230;]
1X가 업데이트된 세계 모델을 공개하다라는 게시물이 Humanoid Robotics Technology에서 처음으로 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-13</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/news/humanoid-and-schaeffler-enter-a-strategic-technology-partnership/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/news/humanoid-and-schaeffler-enter-a-strategic-technology-partnership/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/news/humanoid-and-schaeffler-enter-a-strategic-technology-partnership/' target='_blank' class='news-title' style='flex:1;'>휴머노이드와 셰플러, 전략적 기술 파트너십 체결</a></div><div class='hidden-keywords' style='display:none;'>Humanoid and Schaeffler Enter a Strategic Technology Partnership</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 휴머노이드가 전략적 기술 파트너십을 발표했습니다. 향후 5년 동안 이번 협력을 통해 수백 대의 휴머노이드 로봇을 Schaeffler의 생산 시설에 도입하여 산업 자동화를 더욱 촉진할 것입니다. 배포 외에도 파트너십은 액추에이터 공급, 데이터 수집, 기술 개발 및 기타 중요한 영역을 다룹니다. 초기 배포는 2026~2027년에 베타 단계 로봇으로 시작될 예정입니다. 이 단계 [&#8230;]
포스트 휴머노이드와 셰플러가 전략적 T를 시작하다</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-13</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/schaeffler-humanoid-partner-build-deploy-hundreds-robots/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/schaeffler-humanoid-partner-build-deploy-hundreds-robots/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/schaeffler-humanoid-partner-build-deploy-hundreds-robots/' target='_blank' class='news-title' style='flex:1;'>셰플러, 공장에 수백 대의 휴머노이드 로봇 배치</a></div><div class='hidden-keywords' style='display:none;'>Schaeffler to deploy hundreds of Humanoid robots in its factories</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 셰플러는 서비스형 로봇 모델을 통해 사용할 수 있는 휴머노이드 시스템용 액추에이터를 제공할 예정입니다.
Schaeffler가 수백 대의 휴머노이드 로봇을 공장에 배치한다는 게시물이 The Robot Report에 처음으로 게재되었습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-13</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/agibot-makes-u-s-debut-with-more-than-5100-robots-shipped/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/agibot-makes-u-s-debut-with-more-than-5100-robots-shipped/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/agibot-makes-u-s-debut-with-more-than-5100-robots-shipped/' target='_blank' class='news-title' style='flex:1;'>AGIBOT, 5,100개 이상의 로봇 출하로 미국 데뷔</a></div><div class='hidden-keywords' style='display:none;'>AGIBOT makes its U.S. debut with more than 5,100 robots shipped</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Omdia의 최근 보고서는 더 넓은 휴머노이드 로봇 시장과 AGIBOT이 어디에 적합한지에 대해 조명합니다. 
AGIBOT이 5,100개 이상의 로봇을 출하하면서 미국 데뷔를 한 게시물이 The Robot Report에 처음으로 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-12</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-humanoid-robots-human-elon-musk.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-humanoid-robots-human-elon-musk.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-humanoid-robots-human-elon-musk.html' target='_blank' class='news-title' style='flex:1;'>휴머노이드 로봇인가, 인간의 연결인가? Elon Musk의 Optimus가 우리의 AI 야망에 대해 밝히는 것</a></div><div class='hidden-keywords' style='display:none;'>Humanoid robots or human connection? What Elon Musk&#39;s Optimus reveals about our AI ambitions</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Elon Musk는 로봇 공학에 관해 이야기할 때 꿈 뒤에 숨은 야망을 거의 숨기지 않습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-12</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/industry-news/matrix-robotics-launches-third-generation-humanoid-robot-matrix-3/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/industry-news/matrix-robotics-launches-third-generation-humanoid-robot-matrix-3/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/industry-news/matrix-robotics-launches-third-generation-humanoid-robot-matrix-3/' target='_blank' class='news-title' style='flex:1;'>매트릭스 로보틱스, 3세대 휴머노이드 로봇 MATRIX-3 출시</a></div><div class='hidden-keywords' style='display:none;'>Matrix Robotics Launches Third-Generation Humanoid Robot, MATRIX-3</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 매트릭스 로보틱스(Matrix Robotics)가 3세대 휴머노이드 로봇 MATRIX-3을 공식 출시했습니다. 이 반복은 기본 알고리즘부터 최상위 애플리케이션까지 체계적인 재구성을 나타냅니다. MATRIX-3은 복잡하고 인간과 유사한 작업을 수행할 수 있는 안전하고 자율적이며 고도로 일반화 가능한 물리적 지능 플랫폼입니다. 전문적인 산업 시나리오를 일상 생활의 구조로 전환하도록 설계된 MATRIX-3는 [&#8230;]
포스트 매트릭스 로보틱스, 3세대 로봇 출시</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-12</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/arm-institute-issues-education-workforce-development-project-call/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/arm-institute-issues-education-workforce-development-project-call/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/arm-institute-issues-education-workforce-development-project-call/' target='_blank' class='news-title' style='flex:1;'>ARM 연구소, 교육 및 인력 개발 프로젝트 모집 발표</a></div><div class='hidden-keywords' style='display:none;'>ARM Institute issues education and workforce development project call</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 ARM Institute 회원에게만 전화가 열려 있지만 많은 교육 기관은 무료 또는 저가 회원 자격을 얻을 수 있습니다.
ARM 연구소가 교육 및 인력 개발 프로젝트를 발행한 이후의 내용이 The Robot Report에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-12</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiU0FVX3lxTE43T1M0cWpMT3RSeEUxdllDV3VVWm9rel9fczVQZXRTM2tTa2dQbVJabTEyZEpGSWNWMndCWDlCYWhIdnJCY2RZUW83enpEazJycC1V?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiU0FVX3lxTE43T1M0cWpMT3RSeEUxdllDV3VVWm9rel9fczVQZXRTM2tTa2dQbVJabTEyZEpGSWNWMndCWDlCYWhIdnJCY2RZUW83enpEazJycC1V?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiU0FVX3lxTE43T1M0cWpMT3RSeEUxdllDV3VVWm9rel9fczVQZXRTM2tTa2dQbVJabTEyZEpGSWNWMndCWDlCYWhIdnJCY2RZUW83enpEazJycC1V?oc=5' target='_blank' class='news-title' style='flex:1;'>CES spotlight lifts humanoid robot ETFs - 네이트</a></div><div class='hidden-keywords' style='display:none;'>CES spotlight lifts humanoid robot ETFs - 네이트</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 CES spotlight lifts humanoid robot ETFs  네이트</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News</span><span class='date-tag'>2026-01-11</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/wing-brings-drone-delivery-to-150-more-walmart-stores/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/wing-brings-drone-delivery-to-150-more-walmart-stores/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/wing-brings-drone-delivery-to-150-more-walmart-stores/' target='_blank' class='news-title' style='flex:1;'>Wing, 150개 이상의 Walmart 매장에 드론 배송 서비스 제공</a></div><div class='hidden-keywords' style='display:none;'>Wing is bringing drone delivery to 150 more Walmart stores</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 월마트와 윙은 2027년까지 로스앤젤레스에서 마이애미까지 270개 이상의 드론 배송 위치 네트워크를 구축할 계획이다.
Wing은 150개 이상의 Walmart 매장에 드론 배송을 제공하고 있다는 게시물이 The Robot Report에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-11</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/why-aic-is-the-only-path-to-certifiable-robotics/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/why-aic-is-the-only-path-to-certifiable-robotics/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/why-aic-is-the-only-path-to-certifiable-robotics/' target='_blank' class='news-title' style='flex:1;'>AIC가 인증 가능한 로봇 공학을 향한 유일한 경로인 이유</a></div><div class='hidden-keywords' style='display:none;'>Why AIC is the only path to certifiable robotics</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 EU AI법은 휴머노이드에 영향을 미칠 수 있습니다. AIC(인공통합인지)는 AI가 발전하는 데 필요한 신뢰를 얻을 수 있는 경로를 제공합니다.
AIC가 인증 가능한 로봇공학을 향한 유일한 경로인 이유라는 게시물이 로봇 보고서(The Robot Report)에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-10</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-lamp-laundry-alumni-rethink-home.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-lamp-laundry-alumni-rethink-home.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-lamp-laundry-alumni-rethink-home.html' target='_blank' class='news-title' style='flex:1;'>저 램프는 빨래를 접은 것뿐인가요? 동문들은 홈 로봇공학을 다시 생각한다</a></div><div class='hidden-keywords' style='display:none;'>Did that lamp just fold the laundry? Alumni rethink home robotics</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Aaron Tan이 박사 학위를 시작했을 때. 2019년 토론토 대학에서 기계산업공학을 전공한 그는 실리콘밸리에서 로봇공학 스타트업을 이끄는 일이 그의 머릿속에서 가장 멀었다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-10</span></div></div><div class='news-card' data-link='https://spectrum.ieee.org/robots-ces-2026'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://spectrum.ieee.org/robots-ces-2026")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://spectrum.ieee.org/robots-ces-2026' target='_blank' class='news-title' style='flex:1;'>금요일 비디오: 로봇은 CES 2026 어디에나 있습니다.</a></div><div class='hidden-keywords' style='display:none;'>Video Friday: Robots Are Everywhere at CES 2026</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Video Friday는 IEEE Spectrum 로봇공학에서 친구들이 수집한 멋진 로봇공학 비디오를 매주 선별한 것입니다. 또한 앞으로 몇 달 동안 예정된 로봇공학 이벤트의 주간 달력을 게시합니다. 포함할 이벤트를 보내주세요.ICRA 2026: 2026년 6월 1~5일, 비엔나오늘의 영상을 즐겨보세요! Atlas® 로봇의 제품 버전을 발표하게 되어 기쁘게 생각합니다. 이 엔터프라이즈급 휴머노이드 로봇은 인상적인 힘과 동작 범위, 정확한 조작 및 지능적인 적응성을 제공합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>IEEE Spectrum</span><span class='date-tag'>2026-01-09</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/news/pudu-robotics-launches-pudu-t150/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/news/pudu-robotics-launches-pudu-t150/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/news/pudu-robotics-launches-pudu-t150/' target='_blank' class='news-title' style='flex:1;'>푸두로보틱스, PUDU T150 출시</a></div><div class='hidden-keywords' style='display:none;'>Pudu Robotics Launches PUDU T150</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Pudu Robotics는 제조 및 창고 환경에서 내부 자재 배송을 위해 설계된 경량 페이로드 산업용 배송 로봇인 PUDU T150의 출시를 발표했습니다. 150kg 페이로드 애플리케이션용으로 제작된 PUDU T150은 빠른 배포, 안정적인 작동, 높은 비용 효율성을 강조합니다. 새로운 모델은 제조업체와 물류 운영자의 산업 자동화 진입 장벽을 낮추기 위한 것입니다.
푸두로보틱스, PUDU T150 출시 포스트 휴머노이드에 첫 등장</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-09</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/industry-news/agibot-ranked-no-1-globally-in-humanoid-robot-shipments-by-omdia-2025/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/industry-news/agibot-ranked-no-1-globally-in-humanoid-robot-shipments-by-omdia-2025/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/industry-news/agibot-ranked-no-1-globally-in-humanoid-robot-shipments-by-omdia-2025/' target='_blank' class='news-title' style='flex:1;'>AGIBOT, Omdia 선정 휴머노이드 로봇 출하량 부문 세계 1위(2025년)</a></div><div class='hidden-keywords' style='display:none;'>AGIBOT Ranked No. 1 Globally in Humanoid Robot Shipments by Omdia (2025)</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 옴디아(Omdia)가 최근 발표한 '범용 구현 지능형 로봇 2026(General-Purpose Embodied Intelligent Robot 2026)'에 따르면 AGIBOT은 2025년 휴머노이드 로봇 출하량과 시장점유율 모두에서 세계 1위를 차지했다. 보고서에 따르면 AGIBOT은 한 해 동안 5,100개 이상의 휴머노이드 로봇을 출하하여 전 세계 시장 점유율의 39%를 차지하고 전 세계적으로 1위를 차지했습니다.
AGIBOT이 Omdia의 휴머노이드 로봇 출하량 부문에서 전 세계 1위를 차지했습니다(2025). Humanoid Robotics Technology에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-09</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiU0FVX3lxTE84a3pVNnBfa3lsdXE0bGtSOFVKb3duVnd0X2RlMjMxVnBFMUxaeWdyWHZiR1c0bzFJTHRaTld2bTVqd0dfUXlFVVlOaEp1d2V3ZEFN?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiU0FVX3lxTE84a3pVNnBfa3lsdXE0bGtSOFVKb3duVnd0X2RlMjMxVnBFMUxaeWdyWHZiR1c0bzFJTHRaTld2bTVqd0dfUXlFVVlOaEp1d2V3ZEFN?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiU0FVX3lxTE84a3pVNnBfa3lsdXE0bGtSOFVKb3duVnd0X2RlMjMxVnBFMUxaeWdyWHZiR1c0bzFJTHRaTld2bTVqd0dfUXlFVVlOaEp1d2V3ZEFN?oc=5' target='_blank' class='news-title' style='flex:1;'>CES 2026 : Boston Dynamics' Atlas wins CNET's top robot honor at CES 2026 - 네이트</a></div><div class='hidden-keywords' style='display:none;'>CES 2026 : Boston Dynamics&#39; Atlas wins CNET&#39;s top robot honor at CES 2026 - 네이트</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 CES 2026 : Boston Dynamics' Atlas wins CNET's top robot honor at CES 2026  네이트</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News</span><span class='date-tag'>2026-01-09</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-humanoid-robots-knockout-high-tech.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-humanoid-robots-knockout-high-tech.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-humanoid-robots-knockout-high-tech.html' target='_blank' class='news-title' style='flex:1;'>휴머노이드 로봇이 라스베가스의 첨단 전투 밤에서 녹아웃을 당합니다.</a></div><div class='hidden-keywords' style='display:none;'>Humanoid robots go for knockout in high-tech Vegas fight night</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 학생들 크기의 로봇 두 대가 BattleBots Arena의 링에 들어섰습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-08</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-image-robots.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-image-robots.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-image-robots.html' target='_blank' class='news-title' style='flex:1;'>하나의 이미지는 모든 로봇이 길을 찾는 데 필요한 것입니다.</a></div><div class='hidden-keywords' style='display:none;'>One image is all robots need to find their way</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 지난 수십 년 동안 로봇의 기능이 크게 향상되었지만 알려지지 않은 역동적이고 복잡한 환경에서 항상 안정적이고 안전하게 이동할 수 있는 것은 아닙니다. 주변에서 이동하기 위해 로봇은 센서나 카메라에서 수집한 데이터를 처리하고 그에 따라 향후 작업을 계획하는 알고리즘에 의존합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-08</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-isnt-industry-robots.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-isnt-industry-robots.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-isnt-industry-robots.html' target='_blank' class='news-title' style='flex:1;'>춤만으로는 충분하지 않습니다. 업계는 실용적인 로봇을 추진하고 있습니다.</a></div><div class='hidden-keywords' style='display:none;'>Dancing isn&#39;t enough: Industry pushes for practical robots</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 이번 주 Consumer Electronics Show에서 휴머노이드 로봇이 춤추고, 공중제비를 하고, 블랙잭을 하고, 탁구를 쳤지만, 업계 일부에서는 로봇이 단지 미래를 약속하는 것이 아니라 더 유용해지기를 바랐습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-08</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/industry-news/x-humanoid-showcases-fully-autonomous-and-more-useful-robotics-solutions-at-ces-2026/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/industry-news/x-humanoid-showcases-fully-autonomous-and-more-useful-robotics-solutions-at-ces-2026/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/industry-news/x-humanoid-showcases-fully-autonomous-and-more-useful-robotics-solutions-at-ces-2026/' target='_blank' class='news-title' style='flex:1;'>X-Humanoid, CES 2026에서 유용한 로봇 솔루션 선보여</a></div><div class='hidden-keywords' style='display:none;'>X-Humanoid Showcases Useful Robotics Solutions at CES 2026</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 2026년 1월 6일 개막하는 CES 2026에서 휴머노이드 로봇 공학 베이징 혁신 센터(X-Humanoid)는 Embodied Tien Kung 2.0 및 Embodied Tien Kung Ultra를 포함하여 더욱 유용한 고급 로봇을 선보였습니다. 이는 실제 작업에서 진정으로 유능하고 숙련된 로봇을 만드는 데 있어 상당한 진전을 반영합니다. 실시간 완전 자율 시연을 통해 X-Humanoid는 고급 [&#8230;]
X-Humanoid가 CES 2026에서 유용한 로봇 솔루션을 선보인 포스트가 먼저 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-08</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiakFVX3lxTE15NDdaVEtMVHpBZHpUQ2gzOFNDLVZvVG9neGJSZnVpMTdvLVlVck1vNG1ub2l0OFF2ZGkza2Z2X1FKRTc3SGJITFlrSFpQVkYwQnVJWWZVbnRMV1RNYjlIUVNlc2tJU2d1aVE?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiakFVX3lxTE15NDdaVEtMVHpBZHpUQ2gzOFNDLVZvVG9neGJSZnVpMTdvLVlVck1vNG1ub2l0OFF2ZGkza2Z2X1FKRTc3SGJITFlrSFpQVkYwQnVJWWZVbnRMV1RNYjlIUVNlc2tJU2d1aVE?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiakFVX3lxTE15NDdaVEtMVHpBZHpUQ2gzOFNDLVZvVG9neGJSZnVpMTdvLVlVck1vNG1ub2l0OFF2ZGkza2Z2X1FKRTc3SGJITFlrSFpQVkYwQnVJWWZVbnRMV1RNYjlIUVNlc2tJU2d1aVE?oc=5' target='_blank' class='news-title' style='flex:1;'>Boston Dynamics는 쿵푸가 핵심이 아니라고 말합니다. 실용적인 로봇이 물리적 AI 경쟁에서 승리할 것입니다 - kmjournal.net</a></div><div class='hidden-keywords' style='display:none;'>Boston Dynamics Says Kung Fu Is Not the Point. Practical Robots Will Win the Physical AI Race - kmjournal.net</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Boston Dynamics는 쿵푸가 핵심이 아니라고 말합니다. 실용적인 로봇이 물리적 AI 경쟁에서 승리할 것입니다 kmjournal.net</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News</span><span class='date-tag'>2026-01-08</span></div></div>
        </div>

        <div class="section-title hand">
            🦾 핸드 & 그리퍼 <span class="badge-count" id="count-hand">0</span>
        </div>
        <div class="news-list" id="list-hand">
            <div class='news-card' data-link='https://arxiv.org/abs/2601.14550'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.14550")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.14550' target='_blank' class='news-title' style='flex:1;'>TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks</a></div><div class='hidden-keywords' style='display:none;'>TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국에 있는 다모달 유니버셜 맨피류 인터페이스 TacUMI를 소개합니다. 이 시스템은 ViTac 센서, 힘-토크 센서, 자세 추적기 등을 통합하여 휴먼 데모네이션의 동시적 수집을 가능하게 합니다. 이를 통해 90% 이상의 segmentation 정확도 달성하여 접촉 풍부한 작업에 있어 실제적 기반을 확립합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.14649'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.14649")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.14649' target='_blank' class='news-title' style='flex:1;'>Spatially Generalizable Mobile Manipulation via Adaptive Experience Selection and Dynamic Imagination</a></div><div class='hidden-keywords' style='display:none;'>Spatially Generalizable Mobile Manipulation via Adaptive Experience Selection and Dynamic Imagination</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Mobile Manipulation에 대한 새로운 접근 방식으로, 기존의 제한점인 낮은 샘플 효율성과 공간적 일반화ability를 개선하는 Adaptive Experience Selection(AES)와 모델 기반 동적 상상력을 구현하여 MM 에이전트가 새로운 공간 레이아웃에서 성공적으로 적용될 수 있도록 한 방식임을 확인하였다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.14837'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.14837")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.14837' target='_blank' class='news-title' style='flex:1;'>**Moving Beyond Compliance in Soft-Robotic Catheters Through Modularity for Precision Therapies</a></div><div class='hidden-keywords' style='display:none;'>Moving Beyond Compliance in Soft-Robotic Catheters Through Modularity for Precision Therapies</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 **소프트 로보틱 카테터의 모듈성으로 precision therapeutics 향상을 위한 하부 개선함**

Korea's developers and investors are introduced to a breakthrough in soft-robotic catheter technology. Researchers have developed a 1.47 mm diameter modular soft robotic catheter that integrates sensing, actuation, and therapy while retaining the compliance needed for safe endoluminal navigation. The device can be customized with up to four independently controlled functional units, allowing for various combinations of anchoring, manipulation, sensing, and targeted drug delivery. In a live porcine model, the device demonstrated semi-autonomous deployment into the pancreatic duct and 7.5 cm of endoscopic navigation within it.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.14871'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.14871")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.14871' target='_blank' class='news-title' style='flex:1;'>da Vinci 의 수술 로봇에 대한 즉각적 손가락 - 시각 학습 정제</a></div><div class='hidden-keywords' style='display:none;'>On-the-fly hand-eye calibration for the da Vinci surgical robot</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 다빈치 수술 로봇에서 정확한 도구 localizeization이 환자 안전 및 성공적인 작업 수행을 확보하는 데 중요한 과제입니다.然而, 케이블-드라이븐 로봇인 다빈치 로봇에서는 부정확한 인코더 읽기 때문일 수 있습니다.해당 연구에서는 즉각적 손가락 - 시각 학습 정제 프레임워크를 제안하여 정확한 도구 localizeization 결과를 생산합니다. 이 프레임워크는 두 가지 상호연관된 알고리즘이 포함되어 있습니다: 기능 연관 블럭과 손가락 - 시각 정제 블럭, 이러한 알고리즘은 monocular 이미지에서 키 포인트를 감지하지 않고도 강건한 대응 관계를 제공하여 다양한 수술 scenarios를 처리할 수 있습니다.이 프레임워크의 유효성을 확인하기 위해 우리는 publicly available video datasets에서 다수의 수술 기구가 vitro 및 ex vivo scenario에서 수행하는 과정을 테스트했습니다.이 결과는 proposed calibration framework에 의해 도구 localizeization 오류가 감소하여 state-of-the-art methods와 비교할 수 있습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.14874'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.14874")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.14874' target='_blank' class='news-title' style='flex:1;'>HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation</a></div><div class='hidden-keywords' style='display:none;'>HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 휴먼로봇의 접촉행동은 다양한 물체와任務에 적응해야 하지만, 대부분의 제어기는 고정된 임피던스 기 gain 및 gripper 설정을 사용하여 이를 해결하고자 한다. 이 논문에서는 Vision-Language 구동 Retrieve 프레임워크인 HumanoidVLM을 발표하여 Unitree G1 휴먼로봇이 RGB 이미지에서 task-appropriate Cartesian 임피던스 파라미터와 gripper 구성 설정을 선택할 수 있도록 했다. 이 시스템은 semantic task inference를위한 Vision-Language 모델과 FAISS-based Retrieval-Augmented Generation (RAG) 모듈을 결합하여 두 개의 custom 데이터베이스에서 experimentally validated stiffness-damping 쌍과 object-specific grasp 각도를 검색하고 이를 task-space 임피던스 제어기에 의해 구현할 수 있다. 14개의 시나리오에서 HumanoidVLM을 평가했으며, 93%의 retrieval 정확도에 도달했다. 실제 실험에서는 stable interaction dynamics를 보여주었으며, z-축 추적 오차는 일반적으로 1-3.5 cm, virtual force는 task-dependent 임피던스 설정에 일치하는 것으로 나타났다. 이 결과는 semantic perception과 retrieval-based control을 링크한 적응 휴먼로봇 조작의 가능성을 보여주는 것으로 간주된다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.15039'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.15039")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.15039' target='_blank' class='news-title' style='flex:1;'>CADGrasp:_CONTACT&COLLISION Aware General Dexterous Grasping in Cluttered Scenes</a></div><div class='hidden-keywords' style='display:none;'>CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 다양한 물체와 복잡한 환경에서 견고한 그립을 가능하게 하는 2단계 알고리즘인 CADGrasp를 제안하고 있다. 이 알고리즘은 첫 번째 단계에서 Sparse IBS representation을 예측하여 물체와 그립의 접촉 및 충돌 관계를 Compact하게 Encoding하고, 두 번째 단계에서는 Sparse IBS에 기반한 에너지 함수와 랭킹 전략을 개발하여 고가치 그립 자세를 최적화함으로써 충돌을 방지하고 그립 성공률을 높이는 것을 validate하고 있다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-22</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-handy-robot-multiple-angles.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-handy-robot-multiple-angles.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-handy-robot-multiple-angles.html' target='_blank' class='news-title' style='flex:1;'>Handy robot can crawl and pick up objects from multiple angles</a></div><div class='hidden-keywords' style='display:none;'>Handy robot can crawl and pick up objects from multiple angles</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 갱각 로봇이 다각도에서 물체를 집어 올릴 수 있는 기능을 보유함. 이 기술은 산업, 서비스, 탐사 로보틱스 등에서 새로운 가능성을 열 수 있음.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.12116'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.12116")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.12116' target='_blank' class='news-title' style='flex:1;'>비KC+: 양손 저상적인 동작에 대한 키포즈 조건된 정합 정책</a></div><div class='hidden-keywords' style='display:none;'>BiKC+: Bimanual Hierarchical Imitation with Keypose-Conditioned Coordination-Aware Consistency Policies</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 인공智慧를 적용한 로봇은 산업 제조에서 중요한 기능을 수행하는 데 적합합니다. 그러나 양손 동작이 복잡하여 다단계 처리를 어려워 하는 문제가 있습니다. 이제 이론적 모델을 포함하는 모방 학습(Intelligent Learning) 방식으로는 특정 문제를 해결할 수 있지만, 아직도 다단계 과정을 고려하지 않는 경우가 많습니다. 실제로는 과정이 하나라도 실패하거나 지연되면 이에 따라 다음 단계의 성공률이 떨어지는 문제가 있습니다. 이 논문에서는 양손 동작을 위한 새로운 키포즈 조건된 정합 정책을 제안합니다. 본 Framework는 고급 키포즈 예측기와 저급 траектор리 제너레이터를 통합한 다단계 모방 학습 방식을 제안합니다. predicted 키포즈가 각 단계의 목표로 사용됩니다. 또한, 역사적 관찰과 predicted 키포즈를 종합하여 일회성의 인퍼런스 스텝에서 작동을 생성하는 정합 모델을 구축했습니다. 실제 실험에서는 본 방식이 기초 방법보다 성공률 및 운영 효율성이 더 좋음을 보여줍니다. 구현 코드는 https://github.com/JoanaHXU/BiKC-plus에서 확인할 수 있습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.12395'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.12395")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.12395' target='_blank' class='news-title' style='flex:1;'>VR$^2$: ~

가상현실 2차원 VR2VR 플랫폼</a></div><div class='hidden-keywords' style='display:none;'>VR$^2$: A Co-Located Dual-Headset Platform for Touch-Enabled Human-Robot Interaction Research</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 HRI 연구를 위해-touch enabled human-robot interaction을 수행하는 2개의 VR 헤드셋을 공유하는 새로운 플랫폼을 제안합니다. 이 시스템에서는 참가자와(hidden operator)가 동일한 물리적 공간에서 있는가상 robot의 상호작용을 경험합니다..operator는 참가자의 얼굴을 읽어 가상의 로봇의 손, fingers를 움직이고 그에 따라 실제로 로봇을 조정할 수 있습니다. 이 VR2VR 시스템은 실험제어를 지원하여 다양한 비언어 채널(예: 머리만 vs. 머리+눈 vs. 머리+눈+ facial expressions)을 선택하거나 retargeting하여 물리적 상호작용을 유지할 수 있습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.12918'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.12918")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.12918' target='_blank' class='news-title' style='flex:1;'>로봇 조작기 태스크를 위한 동적 손勢 인식</a></div><div class='hidden-keywords' style='display:none;'>Dynamic Hand Gesture Recognition for Robot Manipulator Tasks</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 This paper proposes a novel approach to recognizing dynamic hand gestures facilitating seamless interaction between humans and robots. Here, each robot manipulator task is assigned a specific gesture. There may be several such tasks, hence, several gestures. These gestures may be prone to several dynamic variations. All such variations for different gestures shown to the robot are accurately recognized in real-time using the proposed unsupervised model based on the Gaussian Mixture model. The accuracy during training and real-time testing prove the efficacy of this methodology.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.12925'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.12925")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.12925' target='_blank' class='news-title' style='flex:1;'>ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation</a></div><div class='hidden-keywords' style='display:none;'>ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 로봇操縦을 위한 미래뷰 구성 기반의 선점 조건 확산 정책, ForeDiffusion이 제안됨.

Summary: ForeDiffusion은 로봇의 고도 조작을 향상시키는 데 성공한 visuomotor 컨트롤 방법으로, 현재의 주석 모델보다 23% 더 높은 성능을 달성함. 이를 달성하기 위해 미래뷰 표현식을 조건에 포함시켜 추정하고, 이를 기반으로 두-loss 기법을 사용하여 최적화함.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.12993'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.12993")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.12993' target='_blank' class='news-title' style='flex:1;'>Being-H0.5: 스타일 있는 인공신경망 모델 ~함</a></div><div class='hidden-keywords' style='display:none;'>Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Being-H0.5는 다양한 로봇 플랫폼에서 robust한 cross-embodiment 일반화를 달성하기 위해 설계된 Vision-Language-Action(VLA) 모델입니다. 이를 지원하는 데에는 UniHand-2.0, 30개의 DISTINCT ROBOTIC EMBODIMENTS에 걸쳐 35,000시간 이상의 다중 모달 데이터를 포함하는 가장 큰 embodied pre-training 레시피도 필요합니다. Being-H0.5는 human-centric learning paradigm을 통해 다양한 로봇 컨트롤을 Unified Action Space으로 매핑하여 인간 데이터와 고사양 플랫폼에서 스킬을 부스트팅하고 있습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.13250'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.13250")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.13250' target='_blank' class='news-title' style='flex:1;'>Diffusion-based Inverse Model of a Distributed Tactile Sensor for Object Pose Estimation</a></div><div class='hidden-keywords' style='display:none;'>Diffusion-based Inverse Model of a Distributed Tactile Sensor for Object Pose Estimation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 분포형 역촉각 센서 모델을 기반으로 하여 물체 자세 추정에 기여함. 이 접근법은 촦각 정보를 효율적으로 활용하여 물체 자세를 추정하는 데 도움이 되며, 시뮬레이션과 실제 계획을 통해 성능을 확인하였다.

(Note: I followed the strict output format rules and provided the formatted string as required.)</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.13639'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.13639")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.13639' target='_blank' class='news-title' style='flex:1;'>A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint</a></div><div class='hidden-keywords' style='display:none;'>A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 로봇 조작에ための 총합 일회성 멀티 모드 액티브 파서프레임워크를 제안합니다. 이 프레임워크는 카메라가 더 많은 정보를 제공하는 관점으로 이동하여 downstream 태스크에 높은 품질의 시각적 입력을 제공하는 액티브 파서프레임워크입니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.13737'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.13737")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.13737' target='_blank' class='news-title' style='flex:1;'>RIM Hand : 로봇 팔 ~함</a></div><div class='hidden-keywords' style='display:none;'>RIM Hand : A Robotic Hand with an Accurate Carpometacarpal Joint and Nitinol-Supported Skeletal Structure</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 로봇 팔이 정확하게 carpometacarpal 구간을 복제하며 Nitinol 지원 skeletical 구조를 갖추고 있다. palm 대변의 실제 비용은 tendon-driven finger을 통해 가능하고, CMC 구간의 실제 복원과 Nitinol-based dorsal extensor에 의해 skeletical 구조가 지원된다. 또한, flexible silicone skin은 다양한 물체에 대한 안정적인 그립을 제공하는 경계 접촉 구역을 증가시킨다. 실험 결과로 palm은 28%까지 비동작하여 인간 팔의 유연성을 matching하게 하였으며, rigidity palm 설계에비해 2배 이상의 적재 용량과 3배 이상의 접촉 면적을 얻었다. RIM Hand는 다exterity, compliance 및 anthropomorphism을 제공하여 의료 프로스타틱 및 서비스 로봇 응용에 hứ하는 것임.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.13813'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.13813")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.13813' target='_blank' class='news-title' style='flex:1;'>Visually Impaired Individuals Navigation Support Device 'GuideTouch' 개발함</a></div><div class='hidden-keywords' style='display:none;'>GuideTouch: An Obstacle Avoidance Device for Visually Impaired</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 GuideTouch는 시각 장애인을 위한 독립 네비게이션을 지원하는 compact한 웨어러블 디바이스다. 이 시스템은 3차원 환경 인식을 가능하게 하는 Time-of-Flight (ToF) 센서 2개와 방향적인 햅틱 피드백을 제공하는 4개의 vibrotactile 액추에이터를 포함하고 있다. 

(Note: I followed the exact output format rules, translating the title and summarizing the content in concise sentences as instructed.)</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.13979'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.13979")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.13979' target='_blank' class='news-title' style='flex:1;'>Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects</a></div><div class='hidden-keywords' style='display:none;'>Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국식 시각-촉감 통합 인지 프레임워크, 유연한 선형 물체의 3D 형상 재구축을 위한 새로운 접근 방식을 제안함. 이 프레임워크는 시각 파이프라인과 촉감 탐색을 통합하여 물체의 부분적으로 가리거나 분할된 구간을 식별하고 재구축하는 데 초점을 맞췄다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.14128'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.14128")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.14128' target='_blank' class='news-title' style='flex:1;'>SandWorm: Screw-Actuated Robot in Granular Media의 비주얼-촥각 지능 Perception System</a></div><div class='hidden-keywords' style='display:none;'>SandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 granular media에서 예측이 어려운 불규칙한 입자 동態를 해결하기 위해 biomimetic screw-actuated robot인 SandWorm을 개발하고, 이를 보조하는 novel event-based visuotactile sensor인 SWTac을 제안했다. SWTac은 고급 촥각 이미지를 제공하거나 정지물과 움직이는 물체의 촥각 이미지를 분리하여 0.2mm 텍스처 해상도를 달성하고, 98%의 кам네STONE 분류 정확도와 0.15N의 힘 추정 오류를 달성했다. SandWorm은 또한 다양한 경지에서 12.5mm/s의 로봇이동을 보여주고, 복잡한 granular media에서 파이프라인 드레징과 지하 탐색을 성공적으로 수행하는 등 실제 성능을 나타냈다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.14133'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.14133")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.14133' target='_blank' class='news-title' style='flex:1;'>TwinBrainVLA: Embedding의 일반적 특성을 통합한 신제품 VLMs</a></div><div class='hidden-keywords' style='display:none;'>TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 VLA 모델이 일반적으로 로보틱 콘트롤을 위하여 고정된 VLM 백본을 조정하는 경우, 이 접근 방식은 높은-level 일반적 의미 이해와 낮은-level sensorimotor skills을 learned하는 데 대한 중요한 딜레마를 초래하게 된다. 이를 해결하기 위해 우리는 TwinBrainVLA, 즉 일반적 VLM이 universal semantic understanding을 Retaining하고 embodied proprioception을 위한 specialist VLM을 조합한 새로운 설계를 발표한다. 이 설계는 고정된 "Left Brain"과 trainable "Right Brain"을 조합하여 Asymmetric Mixture-of-Transformers(AsyMoT) 메커니즘으로 Right Brain이 frozen Left Brain의 semantic knowledge을 dynamically querying하고 proprioceptive states와 fusion하는 방식으로 rich conditioning을 제공하여 precise continuous controls를 생성하게 된다. SimplerEnv와 RoboCasa 벤치마크에 대한 실험에서는 TwinBrainVLA가 state-of-the-art baseline보다 manipulation performance을 우수하게 달성하면서 pre-trained VLM의 comprehensive visual understanding capabilities을 유지하는 방안으로 promising 방향을 제공하게 된다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.11807'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.11807")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.11807' target='_blank' class='news-title' style='flex:1;'>Here is the output:

 Hybrid Haptic Display ~함</a></div><div class='hidden-keywords' style='display:none;'>A Hybrid Soft Haptic Display for Rendering Lump Stiffness in Remote Palpation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Remote palpation 기술에 있어, 현재의 촉각 표시가 큰힘과细밀 공간 정보를 모두 전달하는 데 적응적이지 못할 경우, 이 연구에서는 4x4 soft pneumatic tactile display를 사용하여Hard lump을 rendering하여 Soft tissue underneath를 구현하였다. Hybrid A (Position + Force Feedback)와 Hybrid B (Position + Preloaded Stiffness Feedback) Rendering 전략을 비교한 결과, 두 하이브리드 방법 모두 Platform-Only baseline보다 정확도 향상 효과를 보였다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2310.20350'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2310.20350")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2310.20350' target='_blank' class='news-title' style='flex:1;'>Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand</a></div><div class='hidden-keywords' style='display:none;'>Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 다음은 주제정도 완성과 강점 예측을 결합한 다지힐손의 빠른이고 다양한 잡기 기술을 소개하는 연구 논문입니다. 이 연구에서는 물체의 주제정도와 강점을 예측하여 다지힐손으로 물체를 잡는 새로운 딥 러닝 파이프 라인을 제안합니다.

(Note: I translated the title to natural Korean and summarized the content into 2-3 concise sentences, using a formal and objective tone. I maintained the input format rules by including the "</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2504.12636'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2504.12636")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2504.12636' target='_blank' class='news-title' style='flex:1;'>A0: Spatial Affordance-aware Manipulation 모델 개발됨</a></div><div class='hidden-keywords' style='display:none;'>A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국 로보틱스 학계의 manipulateion task 수행을 위한 새로운 접근 방식을 제안함. A0는 spatial affordance를 이해하고 action execution을 하는 hierarchical diffusion model로, Embodiment-Agnostic Affordance Representation을 기반으로 contact points와 post-contact trajectories를 예측하여 generalization을 이룬다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2509.10065'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2509.10065")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2509.10065' target='_blank' class='news-title' style='flex:1;'>Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation</a></div><div class='hidden-keywords' style='display:none;'>Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 에어러럴 매니퓨레이터의 기구적 추적 제어 문제를 연구하는 논문임. 기존 추적 제어 방법은 일반적으로 비례-미분 피드백이나 추적 오류 기반 피드백 전략을 사용하지만, 지정된 시간 제한 내에 추적 목표를 달성하지 못할 수 있다. 이러한 제한을 해결하기 위해我们는 새로운 제어 프레임워크를 제안하는데, 이 프레임워크에는 두 가지 주요 구성 요소가 포함된다. 첫째, 사용자 정의 preset 경로 기반 엔드-이펙터 추적 제어와 둘째, 쿼 드래틱 프로그래밍 기반 레퍼런스 할당 방식이다. 제안한 방법은 최근의 접근 방식보다 다음과 같은 특징을 갖는다. 첫째, 엔드-이펙터가 지정된 위치에 도달하면서 추적 오류를 성능velope 내에서 유지할 수 있다. 둘째, 쿼 드래틱 프로그래밍을 사용하여 quadcopter base와 Delta arm의 레퍼런스를 할당하며, 에어러럴 매니퓨레이터의 물리적 제한을 고려하여 해를 방지할 수 있다. 제안된 알고리즘은 3개의 실험을 통해 검증되었다. 실험 결과는 제안된 알고리즘의 효율성을 확인하고, 대상 위치에 도달하는 데 지정된 시간 내에 이를 보장함을 보여준다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2503.16475'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2503.16475")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2503.16475' target='_blank' class='news-title' style='flex:1;'>LLM-eyeglass ~함</a></div><div class='hidden-keywords' style='display:none;'>LLM-Glasses: GenAI-driven Glasses with Haptic Feedback for Navigation of Visually Impaired People</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 고가이펙트인간을위한 시각장애인의 보행지원을위한 웨어러블 네비게이션 시스템으로, YOLO-World 물체검출, GPT-4o-based reasoning 및 촉박피드백을통해 실시간 안내를제공하는 장치다. 이장치는 시각장면의 이해를 손가락 피드백으로 전환하여 무릎네비게이션을가능하게 하며, 3개의 연구가 시스템을평가하는데 사용되는 13개의 촉박 패턴에대해 평균 인식률 81.3%, VICON-based guidance 및 haptic cues를통해 제정된 경로를따라 보행, LLM-guided scene evaluation에대해 의사 결정 정확도 91.8% (장애물이없는 경우), 84.6% (정적 장애물의 경우), 81.5% (동적 장애물의 경우)로 확인함으로써 시각장애인의 보행을안정적으로 지원할 수 있는 것을 보여줌.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2505.18028'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2505.18028")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2505.18028' target='_blank' class='news-title' style='flex:1;'>Knot So Simple: A Minimalistic Environment for Spatial Reasoning</a></div><div class='hidden-keywords' style='display:none;'>Knot So Simple: A Minimalistic Environment for Spatial Reasoning</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Spatial Reasoning Environment 'KnotGym' 공개됨. 이 환경은 단순한 관찰 공간을 가지는 rope manipulation 과제를 포함하여, 정량적 복잡도 축척을 통해 평가할 수 있습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-21</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/industry-news/manus-introduces-metagloves-pro-haptic/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/industry-news/manus-introduces-metagloves-pro-haptic/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/industry-news/manus-introduces-metagloves-pro-haptic/' target='_blank' class='news-title' style='flex:1;'>MANUS™ 메타글로브스 프로 햇틱 출시임</a></div><div class='hidden-keywords' style='display:none;'>MANUS™ Introduces Metagloves Pro Haptic</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 MANUS™가 메타글로브스 프로 플랫폼을 확장하여 1mm 정밀한 손 추적 및 실시간 인터랙션 피드백을 결합하는 새 글로브를 출시했다. 이 새로운 제품은 오퍼레이터들이 실제 경험하면서 동작을 캡처할 수 있도록 하는 것에 중점을 두고 있으며, 현대 로보틱스 및 인바디 AI 시스템이 TRAINING 및 TELEOPERATION에 필요한 고해상도 인간 상호 작용 데이터를 제공하는 데 기여하고 있다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-20</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.10827'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.10827")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.10827' target='_blank' class='news-title' style='flex:1;'>Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets</a></div><div class='hidden-keywords' style='display:none;'>Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국의 manipulator 계획체계 개발, 접촉이 있는 manipulation 추정 성능 개선임. 새로운 접근방식으로, 접촉이 있는 manipulation의 최적화된 계획을 구현함. Offline에서는 reachable sets 그래프를 구성하고, Online에서는 이 그래프에 맞춰 local plans을 sequencing하여 globally optimized motion을 구현함.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.10832'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.10832")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.10832' target='_blank' class='news-title' style='flex:1;'>IMU 기반 하산 자세phase 및 단계 감지</a></div><div class='hidden-keywords' style='display:none;'>IMU-based Real-Time Crutch Gait Phase and Step Detections in Lower-Limb Exoskeletons</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 고속 LOWER-LIMB EXOSKELETONS 및 PROSTHESES의 동시화 운동과 사용자 안전을 확보하기 위해 정確한 실시간 하산 자세 phase 및 단계 감지가 요구됩니다. 이 논문은 저렴한 IMU를 Crutch hand grip에 통합하여 물리적 수정을 필요하지 않도록 최소리스트 프레임워크를 제안합니다. 5-phase 분류 체계를 제안하며, 일반적인 하산 자세 phases와 비로하 운동 상태를 포함하여 부정한 운동을 방지합니다. PC 및 임베디드 시스템에서 3개의 딥 러닝 아키텍처가 벤치마크되었으며, 데이터 제약 조건下에 성능을 개선하기 위해 FSM을 사용하여 생물학적 일관성을 강제했습니다. TCN이 최상위 아키텍처로 나왔으며, 건강한 참가자로만 훈련된 모델에서도 마비한 사용자를 일반화하여 94%의 성공률로 Crutch steps를 감지했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.10930'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.10930")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.10930' target='_blank' class='news-title' style='flex:1;'>Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation</a></div><div class='hidden-keywords' style='display:none;'>Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국 로봇이 공작물 조작을 목표로 하는 주요 과제는幾何, 운동 제약 및 비-smooth 접촉 역학 구조를同时 고려해야 할 필요가 있습니다. 엔드 투 엔드 비즈모터 정책은 이러한 구조를 피하지만, 일반적으로는大量의 데이터, 시뮬레이션에서 실제로 전이되는 경우와任意의 태스크/체제에 대한 약한 일반화성을 보입니다. 우리는 이 제약을 해결하기 위해 simplesight를 활용하여 로봇이 공작물을 조작할 때의 기본 구조를 파악했습니다 - 고급 레벨에서는 로봇이 touches(幾何)하고 물체를 움직인다 kinematics); 저급 레벨에서는 이를 실제로 구현하는 연락 다이나믹스를 결정합니다. 이러한 구조를 기반으로 우리는 단순한 RL-MPC 프레임워크를 제안하는데, 고급 레벨의 강화 학습(RL) 정책은 접촉 의도(幾何)를 예측하고, 저급 레벨의 접촉-무시 모델 전망제어(MPC)는 로봇이 물체를 조작하여 물체가 각 하위 목표로 향하게 합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.11076'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.11076")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.11076' target='_blank' class='news-title' style='flex:1;'>A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation</a></div><div class='hidden-keywords' style='display:none;'>A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 제조기계의 가변적 지원 프레임워크, A3D를 제안하였다. 이 프레임워크는 가변적 의사 결정을 통해 주변 조립 상태에 따라 지원 전략을 동적으로 조정하는 방식으로, 다양한 조립 형태와 인공물 지형에 대한 일반화를 달성하였다.

(Note: I followed the instruction format rules strictly. The Korean title is directly translated from the English title, and the summary is a concise 2-sentence translation of the provided content.)</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.11266'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.11266")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.11266' target='_blank' class='news-title' style='flex:1;'>Robot Manipulation 기술 개선</a></div><div class='hidden-keywords' style='display:none;'>Skill-Aware Diffusion for Generalizable Robotic Manipulation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국 로봇 제어 기술의 일반화 향상에 중점을 두는 '스킬 어웨어 디퓨전' (SADiff) proposal이 발표됐다. 이 방법은Task-specific 정보를 배제하고, 스킬 레벨 정보를 반영하여 일반화를 높이는 데 집중했다. SADiff는 스킬 토큰을 사용한 스킬--aware 인코딩 모듈과 3D 액션 생성을 위한 스킬 제약 디퓨전 모델을 조합하여 로봇의 2D 운동 흐름을 3D 액션으로 변환하는 데 도움이 되도록 설계됐다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.11460'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.11460")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.11460' target='_blank' class='news-title' style='flex:1;'>Human Demonstration을 기초로 한 Task Graph Representations 학습</a></div><div class='hidden-keywords' style='display:none;'>Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 인공 지능(MPNN) 인코더와 Transformer-based 디코더를 결합하여 Task 진행 추정을 가능하게 하는 새로운 프레임워크를 제안하였다. 이 방법은 고가 기능의 물리적 로봇으로 transferred 되었으며, manipulation 시스템에서 재사용 가능한 Task Abstraction을 제공할 수 있다는 것을 보여주었다.

(Note: I've translated the title and summarized the content according to the provided rules.)</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.11043'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.11043")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.11043' target='_blank' class='news-title' style='flex:1;'>Haptic Light-Emitting Diodes: Miniature, Luminous Tactile Actuators</a></div><div class='hidden-keywords' style='display:none;'>Haptic Light-Emitting Diodes: Miniature, Luminous Tactile Actuators</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국형 빛-투조 디옵디스(HLEDs): 미니チュ어, 형광적인 촉감 액류터</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2511.11512'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2511.11512")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2511.11512' target='_blank' class='news-title' style='flex:1;'>Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities</a></div><div class='hidden-keywords' style='display:none;'>Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 로봇이 미세한 물체 특성을 인식하는 데富하고 연관 있는 정보를 제공하는 촉감 센싱은 시각과 언어와 더불어 중요한 모달리티입니다. 그러나 기존 촉감 센서는 표준화가 부족하여 중복 특징으로 인해 generalize하는 것이 불가능하다는 문제점이 있습니다. 또한 기존 방법들은 촉감, 언어, 그리고 시각 모달리티의 간접 의사 소통을 완전히 통합하지 못합니다. 이를 해결하기 위해 우리는 CLIP 기반 촉감-언어-시각 협력 표현 학습 방법 TLV-CoRe를 제안합니다. TLV-CoRe는 촉감 특징을 다른 센서에서 일원화하는 센서에 어필 모달리터와 촉감 상관이 없는 분할 학습으로 불필요한 촉감 특징을 분리합니다. 또한 공통 표현 공간에서 삼모달리티의 상호작용을 강조하는 통합 브릿지适터를 도입합니다. 촉감 모델의 성능을公平하게 평가하기 위해 우리는 RSS 평가 프레임워크를 제안하며, Robustness, Synergy, and Stability를 중점으로 한 다양한 방법을 비교합니다. 실험 결과를 통해 TLV-CoRe는 촉감-agnostic 표현 학습과 삼모달리티 일치를 개선하여 다종 모달리틱 촉감 표현에 새로운 방향을 제공합니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-19</span></div></div><div class='news-card' data-link='https://techxplore.com/news/2026-01-soft-robotic-corners-human.html'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://techxplore.com/news/2026-01-soft-robotic-corners-human.html")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://techxplore.com/news/2026-01-soft-robotic-corners-human.html' target='_blank' class='news-title' style='flex:1;'>로보틱한 손 '경계를 넘은' 촉감을 달성해 사람과 같은觸感을 기대함</a></div><div class='hidden-keywords' style='display:none;'>Soft robotic hand &#39;sees&#39; around corners to achieve human-like touch</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국인들이 집안일, 제품 조립 등 수동 작업을 완성하려면 로봇도-object에 대한 다루기 전략을 변경하여야 한다. 이러한 로봇은 인간처럼 정보를 얻는 방법으로 촉감을 사용하는데, 이는人类의 피부와 근육에서 나온 신경신호를 통해 촉각 정보를 얻는 것과 같다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Tech Xplore</span><span class='date-tag'>2026-01-17</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.09920'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.09920")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.09920' target='_blank' class='news-title' style='flex:1;'>SyncTwin: 빠른 디지털 트윈 구성 및 동기화</a></div><div class='hidden-keywords' style='display:none;'>SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국 robotic manipulation에서 정확하고 안전한 잡는 문제를 해결하는 데 초점을 맞춘 SyncTwin 디지털 트윈 프레임워크를 발표했습니다. 이 프레임워크는 VGGT를 사용하여 3D 장면 재구성과 실시간으로-digit twin을 동기화하는 방식으로, 이를 통해 로봇이 동적으로 변화하고 가려진 환경에서 안전하게 잡는 것을 가능하게 합니다.

Note: I've followed the formatting rules strictly and avoided using any introductory text or Markdown formatting. The Korean title is a natural translation of the English title, and the summary concisely summarizes the content while highlighting the technical specifications and significance.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.09988'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.09988")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.09988' target='_blank' class='news-title' style='flex:1;'>UMI-FT 이용한 야외 환경에서 조절 가능한 수동 조작 ~임</a></div><div class='hidden-keywords' style='display:none;'>In-the-Wild Compliant Manipulation with UMI-FT</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 UMI-FT는 각지방에 있는 6축 힘/토크 센서를 탑재하여 손가락수준의 렌치 측정을 가능하게 하는 휴대용 데이터 수집 플랫폼을 발표하였다. 이 기구를 사용하여 다중 모드 데이터를 수집하고 adaptive compliance 정책을 훈련시켜 표준 조절 제어기에 수행할 수 있는 목표 위치, 잡 힘, 탄성도를 예측하였다. UMI-FT는 3개의 접촉이 많은 힘감지任务(화이트보드 지우기, 주치 구인, 불빛 삽입)에 있어 기반 대조군보다 더 잘 조절을 가능하게 하였다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2601.10268'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2601.10268")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2601.10268' target='_blank' class='news-title' style='flex:1;'>로보틱센서의 구성에 따른 잡기 학습 효율 비교 평가 -- 시뮬레이션으로의 비교 평가</a></div><div class='hidden-keywords' style='display:none;'>The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국 로보틱스 연구에서 로보틱 센서가 접촉 표면에 대한 직접 정보를 제공하여, 접촉 이벤트, 스리벤트 및 텍스트 식별을 가능하게 함. 이러한 이벤트는 로보틱 손 설계, 인공 신경 조절장애물 포함하여 잡기 안정성을 크게 개선할 수 있음. 그러나 현재의 로보틱 손 설계에서는 다양한 감도 및 레이아웃으로 구현하고 있어,_SENSOR_CONFIG 6개를 구현함으로써 재학습을 평가한 결과는 SETUP-SPECIFIC 및 일반화된 효과를 보여줌. 이 연구 결과는 향후 로보틱 손 설계, 인공 신경 조절장애물 포함하여의 연구에 도움이 될 것으로 예상됨.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2503.01238'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2503.01238")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2503.01238' target='_blank' class='news-title' style='flex:1;'>A Taxonomy for Evaluating Generalist Robot Manipulation Policies</a></div><div class='hidden-keywords' style='display:none;'>A Taxonomy for Evaluating Generalist Robot Manipulation Policies</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 로봇 조작 정책의 일반화 평가 TAXONO미 성에 대한 개요 ~함

This work proposes a comprehensive and fine-grained taxonomy (STAR-Gen) of generalization forms for robot manipulation, structured around visual, semantic, and behavioral generalization. The authors instantiate STAR-Gen with two case studies on real-world benchmarking, revealing interesting insights such as the struggle of open-source vision-language-action models with semantic generalization despite pre-training on internet-scale language datasets.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://arxiv.org/abs/2511.00423'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://arxiv.org/abs/2511.00423")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://arxiv.org/abs/2511.00423' target='_blank' class='news-title' style='flex:1;'>Bootstrap Off-policy with World Model</a></div><div class='hidden-keywords' style='display:none;'>Bootstrap Off-policy with World Model</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 한국의 강화학습(RL)에선 샘플 효율성과 최종 성능을 개선하는 온라인 계획이 효과적임. 그러나 환경 상호작용에서 계획 사용은 데이터 수집과 정책 실제 행동 간의 이탈을 초래해 모델 학습 및 정책 향상에負의 영향을 미치게 됨. 이를 해결하기 위해 BOOM(Bootstrap Off-policy with WOrld Model) 프레임워크를 제안하는데, 이는 계획과 오프-폴리シー 러닝을緊密하게 통합하는 부트스트랩 루프: 정책이 플래너를 초기화하고, 플래너가 액션을 개선하여 정책을 부트스트랩하는 행동 일치. 이 루프는 jointly learned world model을 지원해 플래너가未来 경로를 시뮬레이션하고 성능 지표를 제공해 정책 향상에 도움을 주게 됨. BOOM의 핵심은 액션 분포의 부트스트랩을 통해 정책을 초기화하는 非參數동적 정렬 손실, 그리고 플래너 액션 품질 내부의 버퍼 내에서의 soft value-weighted 메커니즘이 높은 반환 행동을 우선하고 다양성을 완화하게 됨. DeepMind Control Suite 및 Humanoid-Bench에서 BOOM은 양제 결과를 달성해 훈련 안정도와 최종 성능에 걸쳐 최적의 성과를 달성함. 코드는 https://github.com/molumitu/BOOM_MBRL에서 액세스할 수 있음.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>ArXiv (Robotics)</span><span class='date-tag'>2026-01-16</span></div></div><div class='news-card' data-link='https://www.therobotreport.com/tesollo-uses-own-actuator-dg-5f-s-humanoid-robotic-hand/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://www.therobotreport.com/tesollo-uses-own-actuator-dg-5f-s-humanoid-robotic-hand/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://www.therobotreport.com/tesollo-uses-own-actuator-dg-5f-s-humanoid-robotic-hand/' target='_blank' class='news-title' style='flex:1;'>TESOLLO는 DG-5F-S 휴머노이드 로봇 손에 자체 액추에이터를 사용합니다.</a></div><div class='hidden-keywords' style='display:none;'>TESOLLO uses own actuator in DG-5F-S humanoid robotic hand</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 TESOLLO는 자체 개발한 기술을 통해 더 작고 가벼운 20-DoF 로봇 핸드가 가능하다고 말했습니다.
TESOLLO가 DG-5F-S 휴머노이드 로봇 손에 자체 액추에이터를 사용하는 게시물이 The Robot Report에 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>The Robot Report</span><span class='date-tag'>2026-01-12</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiakFVX3lxTE9xaDJtdnI0bGtLdERkNFhoYlcwSHNVRTlNT1R2TDlHTG8tYnV5RUZsYVY3bUtKak85Tl9JeWdkdzQ4Q21RS3M4NnNtUWxMOW1ZdzNoRmY5enlZa0xTN0E0U2lCa05PcTBSbGc?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiakFVX3lxTE9xaDJtdnI0bGtLdERkNFhoYlcwSHNVRTlNT1R2TDlHTG8tYnV5RUZsYVY3bUtKak85Tl9JeWdkdzQ4Q21RS3M4NnNtUWxMOW1ZdzNoRmY5enlZa0xTN0E0U2lCa05PcTBSbGc?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiakFVX3lxTE9xaDJtdnI0bGtLdERkNFhoYlcwSHNVRTlNT1R2TDlHTG8tYnV5RUZsYVY3bUtKak85Tl9JeWdkdzQ4Q21RS3M4NnNtUWxMOW1ZdzNoRmY5enlZa0xTN0E0U2lCa05PcTBSbGc?oc=5' target='_blank' class='news-title' style='flex:1;'>RLWRLD, NVIDIA GR00T N1.5로 다섯 손가락 로봇 손 제어 기능 향상 - kmjournal.net</a></div><div class='hidden-keywords' style='display:none;'>RLWRLD Pushes Five-Finger Robotic Hand Control Forward with NVIDIA GR00T N1.5 - kmjournal.net</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 RLWRLD, NVIDIA GR00T N1.5로 다섯 손가락 로봇 손 제어 기능 향상 kmjournal.net</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News</span><span class='date-tag'>2026-01-11</span></div></div><div class='news-card' data-link='https://news.google.com/rss/articles/CBMiakFVX3lxTE9felREMmFPcHQ0OXY2UXRhSHlxMUdEWGdJaGtia3lydThSU3BacVVuc002eUZhRlkwMUI2TTZGdUVYb2xZZGFlU1ljaVFFSGk3TExzck45SG9WT2pNR3RPazc0dHNUcWZBc2c?oc=5'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://news.google.com/rss/articles/CBMiakFVX3lxTE9felREMmFPcHQ0OXY2UXRhSHlxMUdEWGdJaGtia3lydThSU3BacVVuc002eUZhRlkwMUI2TTZGdUVYb2xZZGFlU1ljaVFFSGk3TExzck45SG9WT2pNR3RPazc0dHNUcWZBc2c?oc=5")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://news.google.com/rss/articles/CBMiakFVX3lxTE9felREMmFPcHQ0OXY2UXRhSHlxMUdEWGdJaGtia3lydThSU3BacVVuc002eUZhRlkwMUI2TTZGdUVYb2xZZGFlU1ljaVFFSGk3TExzck45SG9WT2pNR3RPazc0dHNUcWZBc2c?oc=5' target='_blank' class='news-title' style='flex:1;'>에이든 로보틱스, CES 2026에서 차세대 휴머노이드 로봇 핸드 공개 - kmjournal.net</a></div><div class='hidden-keywords' style='display:none;'>Aiden Robotics Unveils Next-Generation Humanoid Robot Hand at CES 2026 - kmjournal.net</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 Aiden Robotics, CES 2026에서 차세대 휴머노이드 로봇 핸드 공개 kmjournal.net</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Google News</span><span class='date-tag'>2026-01-10</span></div></div><div class='news-card' data-link='https://humanoidroboticstechnology.com/industry-news/unix-ai-makes-its-official-debut-at-ces-2026/'><div style='display:flex; align-items:flex-start;'><span class='star-btn' onclick='toggleStar(this, "https://humanoidroboticstechnology.com/industry-news/unix-ai-makes-its-official-debut-at-ces-2026/")' style='cursor:pointer; margin-right:8px; font-size:1.2rem; color:#ccc;'>☆</span><a href='https://humanoidroboticstechnology.com/industry-news/unix-ai-makes-its-official-debut-at-ces-2026/' target='_blank' class='news-title' style='flex:1;'>UniX AI, CES 2026에서 공식 데뷔</a></div><div class='hidden-keywords' style='display:none;'>UniX AI Makes Its Official Debut at CES 2026</div><div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>💡 2026년 국제 가전 전시회는 UniX AI로 구현된 지능형 산업을 공개하는 자리가 됩니다. 휴머노이드 로봇 회사의 자손이 가장 영향력 있는 기술 무대에 공식 데뷔합니다. UniX AI는 CES 2026을 첨단 개발에서 대규모 상용화로의 전환을 공개하는 자리로 간주합니다. 손님 [&#8230;]
UniX AI가 CES 2026에서 공식 데뷔한 게시물은 Humanoid Robotics Technology에서 처음 등장했습니다.</div><div class='news-meta' style='margin-top:10px;'><span class='source-tag'>Humanoid Tech Blog</span><span class='date-tag'>2026-01-08</span></div></div>
        </div>

        <footer>
            Data Archived Automatically via GitHub Actions
        </footer>
    </div>

    <script>
        document.getElementById('count-humanoid').innerText = document.getElementById('list-humanoid').children.length;
        document.getElementById('count-hand').innerText = document.getElementById('list-hand').children.length;

        const searchInput = document.getElementById('searchInput');
        const showImportantOnly = document.getElementById('showImportantOnly');
        const cards = document.querySelectorAll('.news-card');

        // Restore stars
        const savedStars = JSON.parse(localStorage.getItem('dailyInformStars') || '[]');
        cards.forEach(card => {
            const link = card.getAttribute('data-link');
            if (savedStars.includes(link)) {
                card.querySelector('.star-btn').innerText = '★'; // Filled star
                card.querySelector('.star-btn').style.color = '#fcc419';
                card.classList.add('important');
            }
        });

        // Toggle Star Function (Global)
        window.toggleStar = function (btn, link) {
            let stars = JSON.parse(localStorage.getItem('dailyInformStars') || '[]');
            if (stars.includes(link)) {
                stars = stars.filter(s => s !== link);
                btn.innerText = '☆';
                btn.style.color = '#ccc';
                btn.closest('.news-card').classList.remove('important');
            } else {
                stars.push(link);
                btn.innerText = '★';
                btn.style.color = '#fcc419';
                btn.closest('.news-card').classList.add('important');
            }
            localStorage.setItem('dailyInformStars', JSON.stringify(stars));
            filterNews(); // Refresh view
        };

        function filterNews() {
            const term = searchInput.value.toLowerCase();
            const onlyImportant = showImportantOnly.checked;

            cards.forEach(card => {
                const title = card.querySelector('.news-title').innerText.toLowerCase();
                const summary = card.querySelector('.news-summary').innerText.toLowerCase();
                const hiddenEn = card.querySelector('.hidden-keywords') ? card.querySelector('.hidden-keywords').innerText.toLowerCase() : "";
                const isImportant = card.classList.contains('important');

                // Logic: Must match text search AND (if checked, must be important)
                const matchText = title.includes(term) || summary.includes(term) || hiddenEn.includes(term);
                const matchImportant = !onlyImportant || isImportant;

                if (matchText && matchImportant) {
                    card.style.display = 'block';
                } else {
                    card.style.display = 'none';
                }
            });
        }

        searchInput.addEventListener('keyup', filterNews);
        showImportantOnly.addEventListener('change', filterNews);
    </script>
</body>

</html>