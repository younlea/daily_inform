import yfinance as yf
import feedparser
import datetime
import urllib.parse
import time
import json
import os
import re
import requests

# ==========================================
# 1. ì„¤ì • ë° í—¬í¼ í•¨ìˆ˜
# ==========================================
ARCHIVE_FILE = 'news_archive.json'
MAX_ITEMS = 2000
GEMINI_KEY = os.environ.get('GEMINI_API_KEY')

# ëª¨ë¸ í›„ë³´êµ° (2.0ì´ ë°˜ì‘ì´ ìˆì—ˆìœ¼ë¯€ë¡œ ìµœìƒë‹¨ ë°°ì¹˜)
CANDIDATE_MODELS = [
    "gemini-2.0-flash-exp",
    "gemini-1.5-flash",
    "gemini-1.5-flash-latest",
    "gemini-1.5-flash-001",
    "gemini-pro"
]

ACTIVE_MODEL = None

if GEMINI_KEY:
    print(f"âœ… DEBUG: API Key Loaded")
else:
    print("âŒ DEBUG: API Key Missing!")

# â˜…â˜…â˜… ìˆ˜ì •ë¨: 429(ê³¼ë¶€í•˜)ë„ 'ì„±ê³µ'ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì„ íƒí•¨ â˜…â˜…â˜…
def find_working_model():
    print("\nğŸ” AI ëª¨ë¸ ìƒì¡´ í™•ì¸ ì¤‘...")
    
    payload = {"contents": [{"parts": [{"text": "hi"}]}]}
    
    for model in CANDIDATE_MODELS:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
        try:
            print(f"   ğŸ‘‰ Testing '{model}'...", end=" ")
            response = requests.post(
                url,
                headers={"Content-Type": "application/json"},
                params={"key": GEMINI_KEY},
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                print("âœ… ì •ìƒ (200 OK)")
                return model
            elif response.status_code == 429:
                print("âœ… ìƒì¡´ í™•ì¸ (429 ê³¼ë¶€í•˜ - ëŒ€ê¸° í›„ ì‚¬ìš© ê°€ëŠ¥)")
                print("      -> ì´ ëª¨ë¸ì„ ì„ íƒí•˜ê³  ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.")
                time.sleep(5) # ìˆ¨ ê³ ë¥´ê¸°
                return model
            else:
                print(f"âŒ ì‹¤íŒ¨ ({response.status_code})")
                
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ({e})")
            
    return None

if GEMINI_KEY:
    ACTIVE_MODEL = find_working_model()
    if ACTIVE_MODEL:
        print(f"\nğŸ‰ [í™•ì •] ì˜¤ëŠ˜ì˜ ëª¨ë¸: {ACTIVE_MODEL}")
    else:
        print("\nğŸš¨ [ì‹¤íŒ¨] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. (ì˜ì–´ ì›ë¬¸ ì €ì¥)")

def process_news_with_ai(title, snippet):
    fallback_summary = snippet[:300] + ("..." if len(snippet) > 300 else "")
    
    if not GEMINI_KEY or not ACTIVE_MODEL:
        return title, fallback_summary

    prompt = f"""
    Role: Professional Tech Reporter (Korea).
    Task: Translate the title into Korean and summarize the snippet into Korean.
    
    Input Title: {title}
    Input Snippet: {snippet}

    Requirements:
    1. Title: Natural Korean translation.
    2. Summary: 2-3 sentences in Korean. Noun-ending style (e.g., ~í•¨, ~ì„).
    3. Output Format: "KOREAN_TITLE ||| KOREAN_SUMMARY"
    4. Do NOT output anything else. Just the formatted string.
    """

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{ACTIVE_MODEL}:generateContent"
    payload = { "contents": [{ "parts": [{"text": prompt}] }] }
    
    # â˜…â˜…â˜… ë…í•œ ì¬ì‹œë„ ë¡œì§ (429 ëœ¨ë©´ ìµœëŒ€ 3ë²ˆ, 60ì´ˆì”© ëŒ€ê¸°) â˜…â˜…â˜…
    for attempt in range(3):
        try:
            response = requests.post(
                url,
                headers={"Content-Type": "application/json"},
                params={"key": GEMINI_KEY},
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                try:
                    result = response.json()
                    result_text = result['candidates'][0]['content']['parts'][0]['text'].strip()
                    if "|||" in result_text:
                        parts = result_text.split("|||")
                        return parts[0].strip(), parts[1].strip()
                    else:
                        return title, result_text
                except:
                    return title, fallback_summary
            
            elif response.status_code == 429:
                print(f"âš ï¸ Quota Limit! 60ì´ˆ ëŒ€ê¸° ì¤‘... ({attempt+1}/3)")
                time.sleep(60) # 1ë¶„ ê°•ì œ íœ´ì‹
                continue # ë‹¤ì‹œ ì‹œë„
            
            else:
                print(f"âŒ Error {response.status_code}")
                # 404ë©´ ë‹µì´ ì—†ìœ¼ë‹ˆ í¬ê¸°
                if response.status_code == 404:
                    return title, fallback_summary
                time.sleep(5)
                continue

        except Exception as e:
            print(f"âŒ Net Error: {e}")
            time.sleep(5)
            continue
            
    return title, fallback_summary

def clean_html(raw_html):
    cleanr = re.compile('<.*?>')
    text = re.sub(cleanr, '', raw_html)
    return text.replace('&nbsp;', ' ').strip()

def make_sparkline_url(data_list, color):
    if not data_list or len(data_list) < 2: return ""
    subset = data_list[-30:]
    data_str = ",".join([f"{x:.2f}" for x in subset])
    chart_config = f"{{type:'sparkline',data:{{datasets:[{{data:[{data_str}],borderColor:'{color}',borderWidth:2,fill:false,pointRadius:0}}]}}}}"
    return "https://quickchart.io/chart?c=" + urllib.parse.quote(chart_config)

def get_metric_data(ticker, color):
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="1mo")
        if hist.empty: return "N/A", "0.00%", ""
        current = hist['Close'].iloc[-1]
        prev = hist['Close'].iloc[-2]
        change = current - prev
        change_pct = (change / prev) * 100
        sign = "+" if change >= 0 else ""
        css_class = "text-red" if change >= 0 else "text-blue"
        val_str = f"{current:,.2f}"
        if ticker == "KRW=X": val_str += " ì›"
        change_str = f"<span class='{css_class}'>{sign}{change:.2f} ({sign}{change_pct:.2f}%)</span>"
        chart_url = make_sparkline_url(hist['Close'].tolist(), color)
        return val_str, change_str, chart_url
    except: return "Error", "-", ""

def load_archive():
    if os.path.exists(ARCHIVE_FILE):
        with open(ARCHIVE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def save_archive(data):
    data.sort(key=lambda x: x['date'], reverse=True)
    if len(data) > MAX_ITEMS: data = data[:MAX_ITEMS]
    with open(ARCHIVE_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# ==========================================
# 2. ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘
# ==========================================
print("1. ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘...")
kospi_val, kospi_chg, kospi_chart = get_metric_data("^KS11", "red")
sp500_val, sp500_chg, sp500_chart = get_metric_data("^GSPC", "red")
usdkrw_val, usdkrw_chg, usdkrw_chart = get_metric_data("KRW=X", "green")

korea_tickers = [
    ('005930.KS', 'ì‚¼ì„±ì „ì', '005930'), ('000660.KS', 'SKí•˜ì´ë‹‰ìŠ¤', '000660'),
    ('373220.KS', 'LGì—ë„ˆì§€ì†”ë£¨ì…˜', '373220'), ('207940.KS', 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', '207940'),
    ('005380.KS', 'í˜„ëŒ€ì°¨', '005380'), ('005490.KS', 'POSCOí™€ë”©ìŠ¤', '005490'),
    ('000270.KS', 'ê¸°ì•„', '000270'), ('035420.KS', 'NAVER', '035420')
]
korea_table_html = "<table class='stock-table'><thead><tr><th>ì¢…ëª©ëª…</th><th>í˜„ì¬ê°€</th><th>ë“±ë½ë¥ </th><th>ì¶”ì„¸(1ë‹¬)</th></tr></thead><tbody>"
for code, name, naver_code in korea_tickers:
    try:
        stock = yf.Ticker(code)
        hist = stock.history(period="1mo")
        if not hist.empty:
            curr = hist['Close'].iloc[-1]
            prev = hist['Close'].iloc[-2]
            pct = ((curr - prev) / prev) * 100
            if pct > 0: color_cls, sign, line_color = "bg-red-light text-red", "+", "red"
            elif pct < 0: color_cls, sign, line_color = "bg-blue-light text-blue", "", "blue"
            else: color_cls, sign, line_color = "text-gray", "", "gray"
            chart = make_sparkline_url(hist['Close'].tolist(), line_color)
            link_url = f"https://finance.naver.com/item/main.naver?code={naver_code}"
            korea_table_html += f"<tr onclick=\"window.open('{link_url}', '_blank')\" style=\"cursor:pointer;\"><td><span class='stock-name'>{name} ğŸ”—</span><span class='stock-code'>{code}</span></td><td class='stock-price'>{curr:,.0f}ì›</td><td><span class='{color_cls}'>{sign}{pct:.2f}%</span></td><td><img src='{chart}' style='height:30px; width:80px;'></td></tr>"
    except: pass
korea_table_html += "</tbody></table>"

# ==========================================
# 3. ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ì²˜ë¦¬
# ==========================================
print("2. ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° AI ì²˜ë¦¬...")
archive = load_archive()
existing_links = set(item['link'] for item in archive)

rss_economy = [{"url": "https://news.google.com/rss/search?q=stock+market+economy+korea+usa&hl=ko&gl=KR&ceid=KR:ko", "title": "ğŸ“ˆ êµ­ë‚´ì™¸ ì¦ì‹œ", "cat": "economy"}]

rss_humanoid = [
    {"url": "https://news.google.com/rss/search?q=humanoid+robot+(startup+OR+unveiled+OR+prototype+OR+new+model)+-vacuum&hl=ko&gl=KR&ceid=KR:ko", "title": "Google News", "cat": "humanoid"},
    {"url": "https://techxplore.com/rss-feed/robotics-news/", "title": "Tech Xplore", "cat": "humanoid"},
    {"url": "https://spectrum.ieee.org/feeds/topic/robotics.rss", "title": "IEEE Spectrum", "cat": "humanoid"},
    {"url": "https://www.therobotreport.com/feed/", "title": "The Robot Report", "cat": "humanoid"},
    {"url": "http://www.irobotnews.com/rss/all.xml", "title": "ë¡œë´‡ì‹ ë¬¸", "cat": "humanoid"},
    {"url": "https://humanoidroboticstechnology.com/feed/", "title": "Humanoid Tech Blog", "cat": "humanoid"}
]

rss_hand = [
    {"url": "https://news.google.com/rss/search?q=robot+hand+gripper+dexterous+manipulation+tactile+sensor+-vacuum&hl=ko&gl=KR&ceid=KR:ko", "title": "Google News", "cat": "hand"}
]

economy_news_latest = []
for src in rss_economy:
    try:
        feed = feedparser.parse(src["url"], agent="Mozilla/5.0")
        for entry in feed.entries[:4]:
            economy_news_latest.append(entry)
    except: pass

today = datetime.datetime.now()
new_items_count = 0

for src in rss_humanoid + rss_hand:
    try:
        feed = feedparser.parse(src["url"], agent="Mozilla/5.0")
        for entry in feed.entries:
            link = entry.link
            if link in existing_links: continue
            
            pub_dt = today
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                pub_dt = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed))
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                pub_dt = datetime.datetime.fromtimestamp(time.mktime(entry.updated_parsed))
            
            if (today - pub_dt).days > 7: continue

            print(f"AI Processing: {entry.title}...")
            raw_snippet = clean_html(entry.get('description', entry.get('summary', '')))
            
            title_ko, summary_ko = process_news_with_ai(entry.title, raw_snippet)
            
            # â˜…â˜…â˜… 2.0 ëª¨ë¸ì€ ë¬´ë£Œ í• ë‹¹ëŸ‰ì´ ì ìœ¼ë¯€ë¡œ 30ì´ˆ ëŒ€ê¸° í•„ìˆ˜ â˜…â˜…â˜…
            print("Cooling down (30s)...")
            time.sleep(30) 

            news_item = {
                "title": title_ko,
                "original_title": entry.title,
                "link": link,
                "date": pub_dt.strftime("%Y-%m-%d %H:%M"),
                "source": src['title'],
                "category": src['cat'],
                "summary": summary_ko
            }
            archive.append(news_item)
            existing_links.add(link)
            new_items_count += 1
            
            # ì•ˆì „í•˜ê²Œ 10ê°œë§Œ
            if new_items_count >= 10:
                print("âš ï¸ ì•ˆì „ì„ ìœ„í•´ ì´ë²ˆ ì‹¤í–‰ì€ 10ê°œê¹Œì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                break
        
        if new_items_count >= 10: break

    except Exception as e:
        print(f"RSS Error: {e}")

save_archive(archive)
print(f"New items: {new_items_count}")

# ==========================================
# 4. HTML ìƒì„±
# ==========================================
print("3. HTML ìƒì„±...")
utc_now = datetime.datetime.now(datetime.timezone.utc)
kst_now = utc_now + datetime.timedelta(hours=9)
now_str = kst_now.strftime("%Y-%m-%d %H:%M:%S (KST)")

# ë©”ì¸ í˜ì´ì§€ (index.html)
def generate_simple_list(items):
    html = ""
    for item in items[:4]:
        title = item.get('title') if isinstance(item, dict) else item.title
        link = item.get('link') if isinstance(item, dict) else item.link
        html += f"<li class='news-item'><a href='{link}' target='_blank'>{title}</a></li>"
    return html

latest_humanoid = [x for x in archive if x['category'] == 'humanoid']
latest_hand = [x for x in archive if x['category'] == 'hand']

main_news_html = ""
if economy_news_latest:
    main_news_html += f"<div class='news-category'><h4><span class='badge'>ğŸ“ˆ ì¦ì‹œ/ê²½ì œ</span></h4><ul class='news-list'>{generate_simple_list(economy_news_latest)}</ul></div>"
if latest_humanoid:
    main_news_html += f"<div class='news-category'><h4><span class='badge'>ğŸ¤– íœ´ë¨¸ë…¸ì´ë“œ</span></h4><ul class='news-list'>{generate_simple_list(latest_humanoid)}</ul></div>"
if latest_hand:
    main_news_html += f"<div class='news-category'><h4><span class='badge'>ğŸ¦¾ í•¸ë“œ/ê·¸ë¦¬í¼</span></h4><ul class='news-list'>{generate_simple_list(latest_hand)}</ul></div>"

with open('template.html', 'r', encoding='utf-8') as f:
    template = f.read()
output_main = template.replace('{{LAST_UPDATED}}', now_str)
output_main = output_main.replace('{{KOSPI_VAL}}', kospi_val).replace('{{KOSPI_CHANGE}}', kospi_chg).replace('{{KOSPI_CHART}}', kospi_chart)
output_main = output_main.replace('{{SP500_VAL}}', sp500_val).replace('{{SP500_CHANGE}}', sp500_chg).replace('{{SP500_CHART}}', sp500_chart)
output_main = output_main.replace('{{USDKRW_VAL}}', usdkrw_val).replace('{{USDKRW_CHANGE}}', usdkrw_chg).replace('{{USDKRW_CHART}}', usdkrw_chart)
output_main = output_main.replace('{{KOREA_MARKET_HTML}}', korea_table_html)
output_main = output_main.replace('{{NEWS_CONTENT}}', main_news_html)

with open('index.html', 'w', encoding='utf-8') as f:
    f.write(output_main)

# ë‰´ìŠ¤ í˜ì´ì§€ (news.html)
def generate_card_list(items):
    html = ""
    for item in items:
        summary_html = f"<div class='news-summary' style='color:#555; font-size:0.95rem; margin-top:8px; line-height:1.6;'>ğŸ’¡ {item.get('summary', '')}</div>" if item.get('summary') else ""
        original_title = item.get('original_title', '').replace("'", "&#39;")
        html += f"""
        <div class='news-card'>
            <a href='{item['link']}' target='_blank' class='news-title'>{item['title']}</a>
            <div class='hidden-keywords' style='display:none;'>{original_title}</div>
            {summary_html}
            <div class='news-meta' style='margin-top:10px;'>
                <span class='source-tag'>{item['source']}</span>
                <span class='date-tag'>{item['date'][:10]}</span>
            </div>
        </div>
        """
    return html

with open('news_template.html', 'r', encoding='utf-8') as f:
    news_template = f.read()

output_news = news_template.replace('{{LAST_UPDATED}}', now_str)
output_news = output_news.replace('{{HUMANOID_NEWS_FULL}}', generate_card_list(latest_humanoid))
output_news = output_news.replace('{{HAND_NEWS_FULL}}', generate_card_list(latest_hand))

with open('news.html', 'w', encoding='utf-8') as f:
    f.write(output_news)

print("ì™„ë£Œ!")
